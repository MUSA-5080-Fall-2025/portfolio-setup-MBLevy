{
  "hash": "667e5fee28302e5ef8517639c6d9b4da",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Assignment 1: Census Data Quality for Policy Decisions\"\nsubtitle: \"Evaluating Data Reliability for Algorithmic Decision-Making\"\nauthor: \"Matthew Levy\"\ndate: today\nformat: \n  html:\n    code-fold: false\n    toc: true\n    toc-location: left\n    theme: cosmo\nexecute:\n  warning: false\n  message: false\n---\n\n# Assignment Overview\n\n## Scenario\n\nYou are a data analyst for the **Pennsylvania Department of Human Services**. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\n\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n## Learning Objectives\n\n- Apply dplyr functions to real census data for policy analysis\n- Evaluate data quality using margins of error \n- Connect technical analysis to algorithmic decision-making\n- Identify potential equity implications of data reliability issues\n- Create professional documentation for policy stakeholders\n\n## Submission Instructions\n\n**Submit by posting your updated portfolio link on Canvas.** Your assignment should be accessible at `your-portfolio-url/assignments/assignment_1/`\n\nMake sure to update your `_quarto.yml` navigation to include this assignment under an \"Assignments\" menu.\n\n# Part 1: Portfolio Integration\n\nCreate this assignment in your portfolio repository under an `assignments/assignment_1/` folder structure. Update your navigation menu to include:\n\n```\n- text: Assignments\n  menu:\n    - href: assignments/assignment_1/your_file_name.qmd\n      text: \"Assignment 1: Census Data Exploration\"\n```\nIf there is a special character like comma, you need use double quote mark so that the quarto can identify this as text\n\n# Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required packages (hint: you need tidycensus, tidyverse, and knitr)\nlibrary(knitr)\nlibrary(tidyverse)\nlibrary(tidycensus)\n# Set your Census API key\ncensus_api_key(\"f6ac8f4f33d6431a4a6a8713369bc824e47dc1e4\")\n# Choose your state for analysis - assign it to a variable called my_state\n```\n:::\n\n\n**State Selection:** I have chosen **Pennsylvania** for this analysis because: I want to get more familiar with the state in which I now live.\n\n\n# Part 2: County-Level Resource Assessment\n\n## 2.1 Data Retrieval\n\n**Your Task:** Use `get_acs()` to retrieve county-level data for your chosen state.\n\n**Requirements:**\n- Geography: county level\n- Variables: median household income (B19013_001) and total population (B01003_001)  \n- Year: 2022\n- Survey: acs5\n- Output format: wide\n\n**Hint:** Remember to give your variables descriptive names using the `variables = c(name = \"code\")` syntax.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Write your get_acs() code here\n\ndataPA <- get_acs(geography = \"county\", \n                  variables = c(popTotal = \"B01003_001\",\n                                medIncome = \"B19013_001\"),\n                  year = 2022, state = \"PA\",\n                  survey = \"acs5\", output = \"wide\")\ndataPA\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 67 × 6\n   GEOID NAME                          popTotalE popTotalM medIncomeE medIncomeM\n   <chr> <chr>                             <dbl>     <dbl>      <dbl>      <dbl>\n 1 42001 Adams County, Pennsylvania       104604        NA      78975       3334\n 2 42003 Allegheny County, Pennsylvan…   1245310        NA      72537        869\n 3 42005 Armstrong County, Pennsylvan…     65538        NA      61011       2202\n 4 42007 Beaver County, Pennsylvania      167629        NA      67194       1531\n 5 42009 Bedford County, Pennsylvania      47613        NA      58337       2606\n 6 42011 Berks County, Pennsylvania       428483        NA      74617       1191\n 7 42013 Blair County, Pennsylvania       122640        NA      59386       2058\n 8 42015 Bradford County, Pennsylvania     60159        NA      60650       2167\n 9 42017 Bucks County, Pennsylvania       645163        NA     107826       1516\n10 42019 Butler County, Pennsylvania      194562        NA      82932       2164\n# ℹ 57 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\ndataPA <- dataPA %>% \n  mutate(\n    countyName = str_remove(NAME, \" County, Pennsylvania\")\n    ) \n# Display the first few rows\nhead(dataPA)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 7\n  GEOID NAME                popTotalE popTotalM medIncomeE medIncomeM countyName\n  <chr> <chr>                   <dbl>     <dbl>      <dbl>      <dbl> <chr>     \n1 42001 Adams County, Penn…    104604        NA      78975       3334 Adams     \n2 42003 Allegheny County, …   1245310        NA      72537        869 Allegheny \n3 42005 Armstrong County, …     65538        NA      61011       2202 Armstrong \n4 42007 Beaver County, Pen…    167629        NA      67194       1531 Beaver    \n5 42009 Bedford County, Pe…     47613        NA      58337       2606 Bedford   \n6 42011 Berks County, Penn…    428483        NA      74617       1191 Berks     \n```\n\n\n:::\n:::\n\n\n## 2.2 Data Quality Assessment\n\n**Your Task:** Calculate margin of error percentages and create reliability categories.\n\n**Requirements:**\n- Calculate MOE percentage: (margin of error / estimate) * 100\n- Create reliability categories:\n  - High Confidence: MOE < 5%\n  - Moderate Confidence: MOE 5-10%  \n  - Low Confidence: MOE > 10%\n- Create a flag for unreliable estimates (MOE > 10%)\n\n**Hint:** Use `mutate()` with `case_when()` for the categories.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MOE percentage and reliability categories using mutate()\ndataPA <- dataPA %>%\n  mutate(moePercentage = (medIncomeM/medIncomeE)*100)\n\ndataPA <- dataPA %>%\n  mutate(reliability = case_when(\n    moePercentage < 5 ~ \"High Confidence\",\n    moePercentage >= 5 & moePercentage <= 10 ~ \"Moderate Confidence\",\n    moePercentage > 10 ~ \"Low Confidence\"))\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\ncount(dataPA, reliability)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  reliability             n\n  <chr>               <int>\n1 High Confidence        57\n2 Moderate Confidence    10\n```\n\n\n:::\n\n```{.r .cell-code}\n#Summary by reliability category\nsumReliability <- dataPA %>% group_by(reliability) %>%\n  summarize(\n    counties = n(),\n    avgIncome = round(mean(medIncomeE,\n                           na.rm = TRUE), 0)\n  )\nsumReliability\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 3\n  reliability         counties avgIncome\n  <chr>                  <int>     <dbl>\n1 High Confidence           57     68090\n2 Moderate Confidence       10     60916\n```\n\n\n:::\n\n```{.r .cell-code}\n#Flag for unreliability\ndataPA <- dataPA %>% mutate(unreliableIncome = case_when(\n  moePercentage < 10 ~ FALSE,\n  moePercentage >= 10 ~ TRUE))\ndataPA\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 67 × 10\n   GEOID NAME               popTotalE popTotalM medIncomeE medIncomeM countyName\n   <chr> <chr>                  <dbl>     <dbl>      <dbl>      <dbl> <chr>     \n 1 42001 Adams County, Pen…    104604        NA      78975       3334 Adams     \n 2 42003 Allegheny County,…   1245310        NA      72537        869 Allegheny \n 3 42005 Armstrong County,…     65538        NA      61011       2202 Armstrong \n 4 42007 Beaver County, Pe…    167629        NA      67194       1531 Beaver    \n 5 42009 Bedford County, P…     47613        NA      58337       2606 Bedford   \n 6 42011 Berks County, Pen…    428483        NA      74617       1191 Berks     \n 7 42013 Blair County, Pen…    122640        NA      59386       2058 Blair     \n 8 42015 Bradford County, …     60159        NA      60650       2167 Bradford  \n 9 42017 Bucks County, Pen…    645163        NA     107826       1516 Bucks     \n10 42019 Butler County, Pe…    194562        NA      82932       2164 Butler    \n# ℹ 57 more rows\n# ℹ 3 more variables: moePercentage <dbl>, reliability <chr>,\n#   unreliableIncome <lgl>\n```\n\n\n:::\n:::\n\n\n## 2.3 High Uncertainty Counties\n\n**Your Task:** Identify the 5 counties with the highest MOE percentages.\n\n**Requirements:**\n- Sort by MOE percentage (highest first)\n- Select the top 5 counties\n- Display: county name, median income, margin of error, MOE percentage, reliability category\n- Format as a professional table using `kable()`\n\n**Hint:** Use `arrange()`, `slice()`, and `select()` functions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create table of top 5 counties by MOE percentage\n\n# Format as table with kable() - include appropriate column names and caption\n\n#Creating a kable table to show unreliability\n#First must create table ordered by MoE\nhighUncertainty <- dataPA %>% \n  arrange(desc(moePercentage)) %>%\n  select(countyName,popTotalE, medIncomeE, moePercentage)\n#only highst MoE\nkable(highUncertainty[1:5,],\n      col.names =  c(\"County\", \"County Population\", \"Median Income\", \n                     \"Income MoE %\"),\n      caption = \"PA Counties with the Highest Income Data Uncertainty\",\n      format.args = list(big.mark = \",\"))\n```\n\n::: {.cell-output-display}\n\n\nTable: PA Counties with the Highest Income Data Uncertainty\n\n|County   | County Population| Median Income| Income MoE %|\n|:--------|-----------------:|-------------:|------------:|\n|Forest   |             6,959|        46,188|     9.985278|\n|Sullivan |             5,880|        62,910|     9.252901|\n|Union    |            42,908|        64,914|     7.321995|\n|Montour  |            18,165|        72,626|     7.085617|\n|Elk      |            30,886|        61,672|     6.633480|\n\n\n:::\n:::\n\n\n**Data Quality Commentary:**\n\n[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty?]\n\nLooking at the counties that ranked the most unreliable in Pennsylvania (Cameron, Sullivan, Montour, Forest, and Union), it becomes immediately apparent that the list is made up of low population counties (including 4 of the top 6 and *all of the top 3*). With this information, I would assume less-populated more-rural counties would not be served well by an algorithm relying on this income data, as these counties' small populations most likely mean that the ACS is gathering their sample from a nearly-irredeemably small number of people.\n\n\n# Part 3: Neighborhood-Level Analysis\n\n## 3.1 Focus Area Selection\n\n**Your Task:** Select 2-3 counties from your reliability analysis for detailed tract-level study.\n\n**Strategy:** Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\n#no low confidence in PA\n\n#Green is favorite color\ncompTibble <- filter(dataPA, GEOID == 42059)\n#7 is my fav number\ncompTibble <- add_row(compTibble, filter(dataPA, reliability == \"High Confidence\")[7, ])\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nkable(compTibble[, c(7, 5, 8,9)],\n      col.names = c(\"County\", \"Median Income\", \"MoE %\",\n                    \"Reliability Category\"),\n      caption = \"A comparison of Income MoE Categories in Pennsylavia\",\n      format.args = list(big.mark = \",\"))\n```\n\n::: {.cell-output-display}\n\n\nTable: A comparison of Income MoE Categories in Pennsylavia\n\n|County | Median Income|    MoE %|Reliability Category |\n|:------|-------------:|--------:|:--------------------|\n|Greene |        66,283| 6.407374|Moderate Confidence  |\n|Blair  |        59,386| 3.465463|High Confidence      |\n\n\n:::\n:::\n\n\n**Comment on the output:** [write something :)]\nAs someone with little knowledge of Pennsylvania outside of Philadelphia, it is hard for me to be too confidence in my observations. However, I noticed once again margin of error seemed to be highly related with population, as Blair county (which has a MoE % of 2.94%) is nearly four times more populated than Greene county (MoE % of 6.31%).\n\n## 3.2 Tract-Level Demographics\n\n**Your Task:** Get demographic data for census tracts in your selected counties.\n\n**Requirements:**\n- Geography: tract level\n- Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001)\n- Use the same state and year as before\n- Output format: wide\n- **Challenge:** You'll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define your race/ethnicity variables with descriptive names\n\n\nvars <- c(\"B03002_003\", \"B03002_004\", \"B03002_012\", \"B03002_001\")\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\n#Changed to philadelphia for later analysis to be possible (Blair andn Greene were both 100% low confidence)\n#GEOIDs didn't work?\n\ndemos <- get_acs(\n  geography = \"tract\",\n  variables = vars,\n  state = \"PA\",\n  county = c(\"Philadelphia\", \"Greene\"),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n#Renaming\ndemos <- demos %>% rename(whiteE = B03002_003E, whiteM = B03002_003M, \n                          aaE = B03002_004E, aaM = B03002_004M,hispE = B03002_012E,\n                          hispM = B03002_012M, totalE = B03002_001E, totalM = B03002_001M)\n\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\ndemos <- demos %>% mutate(whitePerc = 100*(whiteE/totalE),\n                          aaPerc = 100*(aaE/totalE), hispPerc = 100*(hispE/totalE))\n# Add readable tract and county name columns using str_extract() or similar\n\n#Get Census track name and county name through regex\ndemos <- demos %>% mutate(\n  tractName = str_extract(\n    NAME, \"^Census Tract \\\\d+(.\\\\d+)?\"), countyName = substring(str_extract(NAME, \";\\\\s[A-z]+\"),\n       3))\n```\n:::\n\n\n## 3.3 Demographic Analysis\n\n**Your Task:** Analyze the demographic patterns in your selected areas.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\n\n\nhighestHispLat <-demos %>% arrange(desc(hispPerc)) %>% slice(1) %>% select(countyName, tractName)\nprint(highestHispLat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  countyName   tractName          \n  <chr>        <chr>              \n1 Philadelphia Census Tract 195.02\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\nsumDemos <- demos %>% group_by(countyName) %>% summarise(\n  avgWhitePerc = mean(whitePerc, na.rm = TRUE), avgAAPerc = mean(aaPerc, na.rm = TRUE),\n  avgHispLatPerc = mean(hispPerc, na.rm = TRUE)\n)\n\n# Create a nicely formatted table of your results using kable()\nkable(sumDemos,\n      col.names = c(\"County\",\n                    \"Average CT % White Residents\",\n                    \"Average CT % Black Residents\",\n                    \"Average CT % Hispanic/Latine Residents\"),\n      caption = \"A comparison of Average Census Track Demographics in Greene and Philadelphia County, Pennsylvania\",\n      format.args = list(big.mark = \",\"))\n```\n\n::: {.cell-output-display}\n\n\nTable: A comparison of Average Census Track Demographics in Greene and Philadelphia County, Pennsylvania\n\n|County       | Average CT % White Residents| Average CT % Black Residents| Average CT % Hispanic/Latine Residents|\n|:------------|----------------------------:|----------------------------:|--------------------------------------:|\n|Greene       |                     92.56193|                     2.342221|                               1.408517|\n|Philadelphia |                     35.38937|                    39.195000|                              13.843119|\n\n\n:::\n:::\n\n\n# Part 4: Comprehensive Data Quality Evaluation\n\n## 4.1 MOE Analysis for Demographic Variables\n\n**Your Task:** Examine margins of error for demographic variables to see if some communities have less reliable data.\n\n**Requirements:**\n- Calculate MOE percentages for each demographic variable\n- Flag tracts where any demographic variable has MOE > 15%\n- Create summary statistics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\ndemos <- demos %>% mutate(\n  whiteMoEPerc = 100*(whiteM/whiteE),\n  aaMoEPerc = 100*(aaM/aaE),\n  hispLatMoEPerc = 100*(hispM/hispE)\n)\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\ndemos <- demos %>% mutate(\n  demoUnreliable = if_else(whiteMoEPerc > 15 | aaMoEPerc > 15 | hispLatMoEPerc > 15,\n                     TRUE, FALSE)\n)\n# Create summary statistics showing how many tracts have data quality issues\n\ndataIssuesCounty <- demos %>% filter(demoUnreliable == TRUE) %>% summarise(\n  unreliableTracts = n()) %>% mutate(\n    unreliablePercentage = paste(as.character(100*unreliableTracts/nrow(demos)), \"%\"))\ndataIssuesCounty\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  unreliableTracts unreliablePercentage\n             <int> <chr>               \n1              418 100 %               \n```\n\n\n:::\n:::\n\n\n## 4.2 Pattern Analysis\n\n**Your Task:** Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\n#all tracts high MoE so no need for group by\nhighMoESummary <- demos %>% summarise(\n  avgPop = mean(totalE), avgWhitePerc = mean(whitePerc, na.rm = TRUE),\n  avgAAPerc = mean(aaPerc, na.rm = TRUE), avgHispLatPerc = mean(hispPerc, na.rm = TRUE))\n  \n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\nkable(highMoESummary,\n      col.names = c(\"Average Population\", \"Average % White Residents\", \"Average % Black Residents\", \"Average % Hispanic/Latine Residents\"),\n           caption = \"Characteristics of High Margin of Error Census Tracts in Greene and Philadelphia County, Pennsylvaia\",\n      format.args = list(big.mark = \",\"))\n```\n\n::: {.cell-output-display}\n\n\nTable: Characteristics of High Margin of Error Census Tracts in Greene and Philadelphia County, Pennsylvaia\n\n| Average Population| Average % White Residents| Average % Black Residents| Average % Hispanic/Latine Residents|\n|------------------:|-------------------------:|-------------------------:|-----------------------------------:|\n|          3,897.103|                  36.81512|                  38.27598|                            13.53303|\n\n\n:::\n:::\n\n\n**Pattern Analysis:** [Describe any patterns you observe. Do certain types of communities have less reliable data? What might explain this?]\n\nEvery single tract has a margin of error percentage grater than 15. Perhaps it is because dividing census tracks — which don't have large population totals — into demographic groups leaves a very small number of people to sample to forge the ACS's estimates.\n\n# Part 5: Policy Recommendations\n\n## 5.1 Analysis Integration and Professional Summary\n\n**Your Task:** Write an executive summary that integrates findings from all four analyses.\n\n**Executive Summary Requirements:**\n1. **Overall Pattern Identification**: What are the systematic patterns across all your analyses?\n2. **Equity Assessment**: Which communities face the greatest risk of algorithmic bias based on your findings?\n3. **Root Cause Analysis**: What underlying factors drive both data quality issues and bias risk?\n4. **Strategic Recommendations**: What should the Department implement to address these systematic issues?\n\n**Executive Summary:**\n\n[Your integrated 4-paragraph summary here]\n\n\nOverall, it appears that the ACS estimates are far less reliable for smaller population totals. This becomes especially relevant for smaller geographies or narrower groups, which was exemplified by every census tract in both Philadelphia and Greene counties having unreliable margin of errors at the demographic level.\n\n\n\n## 6.3 Specific Recommendations\n\n**Your Task:** Create a decision framework for algorithm implementation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\nsumTable <- dataPA %>% select(countyName, medIncomeE, moePercentage, reliability)\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\nsumTable <- sumTable %>% mutate(algRec = case_when(\n  reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n  reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n  reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n))\n# Format as a professional table with kable()\nkable(sumTable,\n      col.names = c(\"PA County\", \"Median Income Estimate\", \"Margin of Error Percentage\",\n                    \"Reliability\", \"Algorithmic Recommendations\"),\n      caption = \"Algorithmic Recommendations for 5-Year ACS Income Data for Pennsylvania Counties\",\n      format.args = list(big.mark = \",\")\n      )\n```\n\n::: {.cell-output-display}\n\n\nTable: Algorithmic Recommendations for 5-Year ACS Income Data for Pennsylvania Counties\n\n|PA County      | Median Income Estimate| Margin of Error Percentage|Reliability         |Algorithmic Recommendations         |\n|:--------------|----------------------:|--------------------------:|:-------------------|:-----------------------------------|\n|Adams          |                 78,975|                   4.221589|High Confidence     |Safe for algorithmic decisions      |\n|Allegheny      |                 72,537|                   1.198009|High Confidence     |Safe for algorithmic decisions      |\n|Armstrong      |                 61,011|                   3.609185|High Confidence     |Safe for algorithmic decisions      |\n|Beaver         |                 67,194|                   2.278477|High Confidence     |Safe for algorithmic decisions      |\n|Bedford        |                 58,337|                   4.467148|High Confidence     |Safe for algorithmic decisions      |\n|Berks          |                 74,617|                   1.596151|High Confidence     |Safe for algorithmic decisions      |\n|Blair          |                 59,386|                   3.465463|High Confidence     |Safe for algorithmic decisions      |\n|Bradford       |                 60,650|                   3.572960|High Confidence     |Safe for algorithmic decisions      |\n|Bucks          |                107,826|                   1.405969|High Confidence     |Safe for algorithmic decisions      |\n|Butler         |                 82,932|                   2.609367|High Confidence     |Safe for algorithmic decisions      |\n|Cambria        |                 54,221|                   3.336346|High Confidence     |Safe for algorithmic decisions      |\n|Cameron        |                 46,186|                   5.640237|Moderate Confidence |Use with caution - monitor outcomes |\n|Carbon         |                 64,538|                   5.305402|Moderate Confidence |Use with caution - monitor outcomes |\n|Centre         |                 70,087|                   2.769415|High Confidence     |Safe for algorithmic decisions      |\n|Chester        |                118,574|                   1.704421|High Confidence     |Safe for algorithmic decisions      |\n|Clarion        |                 58,690|                   4.365309|High Confidence     |Safe for algorithmic decisions      |\n|Clearfield     |                 56,982|                   2.792110|High Confidence     |Safe for algorithmic decisions      |\n|Clinton        |                 59,011|                   3.855213|High Confidence     |Safe for algorithmic decisions      |\n|Columbia       |                 59,457|                   3.757337|High Confidence     |Safe for algorithmic decisions      |\n|Crawford       |                 58,734|                   3.912555|High Confidence     |Safe for algorithmic decisions      |\n|Cumberland     |                 82,849|                   2.199182|High Confidence     |Safe for algorithmic decisions      |\n|Dauphin        |                 71,046|                   2.271768|High Confidence     |Safe for algorithmic decisions      |\n|Delaware       |                 86,390|                   1.534900|High Confidence     |Safe for algorithmic decisions      |\n|Elk            |                 61,672|                   6.633480|Moderate Confidence |Use with caution - monitor outcomes |\n|Erie           |                 59,396|                   2.550677|High Confidence     |Safe for algorithmic decisions      |\n|Fayette        |                 55,579|                   4.163443|High Confidence     |Safe for algorithmic decisions      |\n|Forest         |                 46,188|                   9.985278|Moderate Confidence |Use with caution - monitor outcomes |\n|Franklin       |                 71,808|                   2.998273|High Confidence     |Safe for algorithmic decisions      |\n|Fulton         |                 63,153|                   3.646699|High Confidence     |Safe for algorithmic decisions      |\n|Greene         |                 66,283|                   6.407374|Moderate Confidence |Use with caution - monitor outcomes |\n|Huntingdon     |                 61,300|                   4.717781|High Confidence     |Safe for algorithmic decisions      |\n|Indiana        |                 57,170|                   4.647542|High Confidence     |Safe for algorithmic decisions      |\n|Jefferson      |                 56,607|                   3.413006|High Confidence     |Safe for algorithmic decisions      |\n|Juniata        |                 61,915|                   4.785593|High Confidence     |Safe for algorithmic decisions      |\n|Lackawanna     |                 63,739|                   2.579269|High Confidence     |Safe for algorithmic decisions      |\n|Lancaster      |                 81,458|                   1.794790|High Confidence     |Safe for algorithmic decisions      |\n|Lawrence       |                 57,585|                   3.068507|High Confidence     |Safe for algorithmic decisions      |\n|Lebanon        |                 72,532|                   2.687090|High Confidence     |Safe for algorithmic decisions      |\n|Lehigh         |                 74,973|                   1.999386|High Confidence     |Safe for algorithmic decisions      |\n|Luzerne        |                 60,836|                   2.350582|High Confidence     |Safe for algorithmic decisions      |\n|Lycoming       |                 63,437|                   4.394912|High Confidence     |Safe for algorithmic decisions      |\n|McKean         |                 57,861|                   4.749313|High Confidence     |Safe for algorithmic decisions      |\n|Mercer         |                 57,353|                   3.633637|High Confidence     |Safe for algorithmic decisions      |\n|Mifflin        |                 58,012|                   3.428601|High Confidence     |Safe for algorithmic decisions      |\n|Monroe         |                 80,656|                   3.173973|High Confidence     |Safe for algorithmic decisions      |\n|Montgomery     |                107,441|                   1.271395|High Confidence     |Safe for algorithmic decisions      |\n|Montour        |                 72,626|                   7.085617|Moderate Confidence |Use with caution - monitor outcomes |\n|Northampton    |                 82,201|                   1.931850|High Confidence     |Safe for algorithmic decisions      |\n|Northumberland |                 55,952|                   2.671933|High Confidence     |Safe for algorithmic decisions      |\n|Perry          |                 76,103|                   3.173331|High Confidence     |Safe for algorithmic decisions      |\n|Philadelphia   |                 57,537|                   1.376506|High Confidence     |Safe for algorithmic decisions      |\n|Pike           |                 76,416|                   4.895572|High Confidence     |Safe for algorithmic decisions      |\n|Potter         |                 56,491|                   4.416633|High Confidence     |Safe for algorithmic decisions      |\n|Schuylkill     |                 63,574|                   2.401925|High Confidence     |Safe for algorithmic decisions      |\n|Snyder         |                 65,914|                   5.561793|Moderate Confidence |Use with caution - monitor outcomes |\n|Somerset       |                 57,357|                   2.775598|High Confidence     |Safe for algorithmic decisions      |\n|Sullivan       |                 62,910|                   9.252901|Moderate Confidence |Use with caution - monitor outcomes |\n|Susquehanna    |                 63,968|                   3.142196|High Confidence     |Safe for algorithmic decisions      |\n|Tioga          |                 59,707|                   3.227427|High Confidence     |Safe for algorithmic decisions      |\n|Union          |                 64,914|                   7.321995|Moderate Confidence |Use with caution - monitor outcomes |\n|Venango        |                 59,278|                   3.448160|High Confidence     |Safe for algorithmic decisions      |\n|Warren         |                 57,925|                   5.187743|Moderate Confidence |Use with caution - monitor outcomes |\n|Washington     |                 74,403|                   2.377592|High Confidence     |Safe for algorithmic decisions      |\n|Wayne          |                 59,240|                   4.794058|High Confidence     |Safe for algorithmic decisions      |\n|Westmoreland   |                 69,454|                   1.991246|High Confidence     |Safe for algorithmic decisions      |\n|Wyoming        |                 67,968|                   3.851813|High Confidence     |Safe for algorithmic decisions      |\n|York           |                 79,183|                   1.788263|High Confidence     |Safe for algorithmic decisions      |\n\n\n:::\n\n```{.r .cell-code}\nkable(highMoESummary,\n      col.names = c(\"Average Population\", \"Average % White Residents\",\n                    \"Average % Black Residents\", \"Average % Hispanic/Latine Residents\"),\n      caption = \n        \"Characteristics of High Margin of Error Census Tracts in Greene and Philadelphia County, Pennsylvaia\",\n      format.args = list(big.mark = \",\"))\n```\n\n::: {.cell-output-display}\n\n\nTable: Characteristics of High Margin of Error Census Tracts in Greene and Philadelphia County, Pennsylvaia\n\n| Average Population| Average % White Residents| Average % Black Residents| Average % Hispanic/Latine Residents|\n|------------------:|-------------------------:|-------------------------:|-----------------------------------:|\n|          3,897.103|                  36.81512|                  38.27598|                            13.53303|\n\n\n:::\n:::\n\n\n**Key Recommendations:**\n\n**Your Task:** Use your analysis results to provide specific guidance to the department.\n\n1. **Counties suitable for immediate algorithmic implementation:** [List counties with high confidence data and explain why they're appropriate]\n\nThe counties of Philadelphia, Allegheny, Bucks, Montgomery, and Berks are the 5 of the 57 \"High Confidence\" counties with the lowest margin of error percentage. Their relatively large populations compared to the rest of the counties most likely means that the ACS samples more residents of these counties than they do people of other counties, which would lower the margin of error. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Full List of high confidence counties\nq1 <- sumTable %>% filter(reliability == \"High Confidence\") %>%\n  arrange(moePercentage)\nprint(q1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 57 × 5\n   countyName   medIncomeE moePercentage reliability     algRec                 \n   <chr>             <dbl>         <dbl> <chr>           <chr>                  \n 1 Allegheny         72537          1.20 High Confidence Safe for algorithmic d…\n 2 Montgomery       107441          1.27 High Confidence Safe for algorithmic d…\n 3 Philadelphia      57537          1.38 High Confidence Safe for algorithmic d…\n 4 Bucks            107826          1.41 High Confidence Safe for algorithmic d…\n 5 Delaware          86390          1.53 High Confidence Safe for algorithmic d…\n 6 Berks             74617          1.60 High Confidence Safe for algorithmic d…\n 7 Chester          118574          1.70 High Confidence Safe for algorithmic d…\n 8 York              79183          1.79 High Confidence Safe for algorithmic d…\n 9 Lancaster         81458          1.79 High Confidence Safe for algorithmic d…\n10 Northampton       82201          1.93 High Confidence Safe for algorithmic d…\n# ℹ 47 more rows\n```\n\n\n:::\n:::\n\n2. **Counties requiring additional oversight:** [List counties with moderate confidence data and describe what kind of monitoring would be needed]\n\nWarren, Carbon, and Snyder are the three \"moderate confidence counties with the lowest margin of error percentage.To ensure reliability, analysts should consult other data sets and local sources to check against the ACS data.\n\n::: {.cell}\n\n```{.r .cell-code}\n#Full List of moderate confidence counties\nq2 <- sumTable %>% filter(reliability == \"Moderate Confidence\") %>%\n  arrange(moePercentage)\nprint(q2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 5\n   countyName medIncomeE moePercentage reliability         algRec               \n   <chr>           <dbl>         <dbl> <chr>               <chr>                \n 1 Warren          57925          5.19 Moderate Confidence Use with caution - m…\n 2 Carbon          64538          5.31 Moderate Confidence Use with caution - m…\n 3 Snyder          65914          5.56 Moderate Confidence Use with caution - m…\n 4 Cameron         46186          5.64 Moderate Confidence Use with caution - m…\n 5 Greene          66283          6.41 Moderate Confidence Use with caution - m…\n 6 Elk             61672          6.63 Moderate Confidence Use with caution - m…\n 7 Montour         72626          7.09 Moderate Confidence Use with caution - m…\n 8 Union           64914          7.32 Moderate Confidence Use with caution - m…\n 9 Sullivan        62910          9.25 Moderate Confidence Use with caution - m…\n10 Forest          46188          9.99 Moderate Confidence Use with caution - m…\n```\n\n\n:::\n:::\n\n3. **Counties needing alternative approaches:** [List counties with low confidence data and suggest specific alternatives - manual review, additional surveys, etc.]\n\nThere are no \"low confidence\" counties in Pennsylvania. If there were, I would recommend conducting additional surveys and putting boots in the local community.\n\n## Questions for Further Investigation\n\n[List 2-3 questions that your analysis raised that you'd like to explore further in future assignments. Consider questions about spatial patterns, time trends, or other demographic factors.]\n\n  - With low population totals seemingly linked to higher margins of error, how do analysts create reliable algorithms for smaller subgroups and micro-categories?\n  - Besides simply arbitrary deciding, how do we determine what the threshold for \"reliability\" is?\n\n\n\n# Technical Notes\n\n**Data Sources:** \n- U.S. Census Bureau, American Community Survey 2018-2022 5-Year Estimates\n- Retrieved via tidycensus R package on [date]\n\n**Reproducibility:** \n- All analysis conducted in R version [your version]\n- Census API key required for replication\n- Complete code and documentation available at: [your portfolio URL]\n\n**Methodology Notes:**\n[Describe any decisions you made about data processing, county selection, or analytical choices that might affect reproducibility]\n\n**Limitations:**\n[Note any limitations in your analysis - sample size issues, geographic scope, temporal factors, etc.]\n\n---\n\n## Submission Checklist\n\nBefore submitting your portfolio link on Canvas:\n\n- [ ] All code chunks run without errors\n- [ ] All \"[Fill this in]\" prompts have been completed\n- [ ] Tables are properly formatted and readable\n- [ ] Executive summary addresses all four required components\n- [ ] Portfolio navigation includes this assignment\n- [ ] Census API key is properly set \n- [ ] Document renders correctly to HTML\n\n**Remember:** Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at `your-portfolio-url/assignments/assignment_1/your_file_name.html`",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}