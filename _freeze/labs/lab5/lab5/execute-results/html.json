{
  "hash": "71161636af7cdfda2e7676ab1c3c82ff",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Space-Time Prediction of Bike Share Demand: Philadelphia Indego\"\nauthor: \"Matthew\"\ndate: \"2025-12-01\"\noutput: \n  html_document:\n    toc: true\n    toc_float: true\n    code_folding: show\n    code_download: true\n---\n\n\n\n# Introduction\n\n## The Rebalancing Challenge in Philadelphia\n\nPhiladelphia's Indego bike share system faces the same operational challenge as every bike share system: **rebalancing bikes to meet anticipated demand**. \n\nImagine you're an Indego operations manager at 6:00 AM on a Monday morning. You have:\n- 200 stations across Philadelphia\n- Limited trucks and staff for moving bikes\n- 2-3 hours before morning rush hour demand peaks\n- **The question:** Which stations will run out of bikes by 8:30 AM?\n\nThis lab will teach you to build predictive models that forecast bike share demand across **space** (different stations) and **time** (different hours) to help solve this operational problem.\n\n## Learning Objectives\n\nBy the end of this assignment, you will be able to:\n\n1. **Understand panel data structure** for space-time analysis\n2. **Create temporal lag variables** to capture demand persistence\n3. **Build multiple predictive models** with increasing complexity\n4. **Validate models temporally** (train on past, test on future)\n5. **Analyze prediction errors** in both space and time\n6. **Engineer new features** based on error patterns\n7. **Critically evaluate** when prediction errors matter most\n\n## Assignment Structure\n\n**Part 1 (In-Class/Together):** Work through Q1 2025 data following this code\n\n**Part 2 (HW5 - Teams of 2):**\n- Download a different quarter of Indego data (Q2, Q3, or Q4 2024)\n- Apply the same workflow to your quarter\n- Analyze error patterns in your quarter\n- Add 2-3 new features to improve the model\n- Write a brief report on what you learned\n\n---\n\n# Setup\n\n## Load Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\n#install.packages(\"tigris\")\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# here!\nlibrary(here)\n\n#for setting data path \nlibrary(\"this.path\")\n\n\n# Get rid of scientific notation. We gotta look good!\noptions(scipen = 999)\n```\n:::\n\n\n## Define Themes\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 <- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n```\n:::\n\n\n## Set Census API Key\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_api_key(\"YOUR_Key_Here\", overwrite = TRUE, install = TRUE)\n```\n:::\n\n\n\n\n---\n\n# Data Import & Preparation\n\n## Load Indego Trip Data (Q1 2025)\n\n**Note for HW5:** You'll download a different quarter from: https://www.rideindego.com/about/data/\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#get current directory (not same as project home)\ncurDir <- dirname(this.path())\nsetwd(curDir)\n# Read Q1 2025 data\n\nindego <- read_csv(here(\"data/indego-trips-2025-q1.csv\"))\n\n# Quick look at the data\nglimpse(indego)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 201,588\nColumns: 15\n$ trip_id             <dbl> 1123985990, 1123986350, 1124202498, 1123986241, 11…\n$ duration            <dbl> 5, 14, 894, 9, 13, 11, 15, 19, 20, 24, 26, 11, 12,…\n$ start_time          <chr> \"1/1/2025 0:00\", \"1/1/2025 0:04\", \"1/1/2025 0:05\",…\n$ end_time            <chr> \"1/1/2025 0:05\", \"1/1/2025 0:18\", \"1/1/2025 14:59\"…\n$ start_station       <dbl> 3371, 3344, 3021, 3253, 3182, 3346, 3049, 3112, 31…\n$ start_lat           <dbl> 39.95340, 39.95961, 39.95390, 39.93074, 39.95081, …\n$ start_lon           <dbl> -75.15430, -75.23354, -75.16902, -75.18924, -75.16…\n$ end_station         <dbl> 3378, 3294, 3073, 3253, 3249, 3000, 3100, 3035, 33…\n$ end_lat             <dbl> 39.95238, 39.95174, 39.96143, 39.93074, 39.95784, …\n$ end_lon             <dbl> -75.14728, -75.17063, -75.15242, -75.18924, -75.19…\n$ bike_id             <chr> \"22580\", \"31417\", \"31393\", \"31434\", \"22872\", \"1187…\n$ plan_duration       <dbl> 30, 365, 1, 30, 1, 365, 30, 30, 365, 365, 30, 365,…\n$ trip_route_category <chr> \"One Way\", \"One Way\", \"One Way\", \"Round Trip\", \"On…\n$ passholder_type     <chr> \"Indego30\", \"Indego365\", \"Walk-up\", \"Indego30\", \"W…\n$ bike_type           <chr> \"electric\", \"electric\", \"electric\", \"electric\", \"e…\n```\n\n\n:::\n:::\n\n\n## Examine the Data Structure\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many trips?\ncat(\"Total trips in Q1 2025:\", nrow(indego), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal trips in Q1 2025: 201588 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego$start_time)), \"to\", \n    max(mdy_hm(indego$start_time)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDate range: 1735689600 to 1743464880 \n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego$start_station)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique start stations: 265 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Trip types\ntable(indego$trip_route_category)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   One Way Round Trip \n    190792      10796 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Passholder types\ntable(indego$passholder_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  Day Pass   Indego30  Indego365 IndegoFlex    Walk-up \n      5494      94044      91628          3      10419 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Bike types\ntable(indego$bike_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nelectric standard \n  129561    72027 \n```\n\n\n:::\n:::\n\n\n## Create Time Bins\n\nWe need to aggregate trips into hourly intervals for our panel data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindego <- indego %>%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego %>% select(start_datetime, interval60, week, dotw, hour, weekend))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  <dttm>              <dttm>              <dbl> <ord> <int>   <dbl>\n1 2025-01-01 00:00:00 2025-01-01 00:00:00     1 Wed       0       0\n2 2025-01-01 00:04:00 2025-01-01 00:00:00     1 Wed       0       0\n3 2025-01-01 00:05:00 2025-01-01 00:00:00     1 Wed       0       0\n4 2025-01-01 00:05:00 2025-01-01 00:00:00     1 Wed       0       0\n5 2025-01-01 00:08:00 2025-01-01 00:00:00     1 Wed       0       0\n6 2025-01-01 00:14:00 2025-01-01 00:00:00     1 Wed       0       0\n```\n\n\n:::\n:::\n\n\n---\n\n# Exploratory Analysis\n\n## Trips Over Time\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daily trip counts\ndaily_trips <- indego %>%\n  group_by(date) %>%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q1 2025\",\n    subtitle = \"Winter demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/trips_over_time-1.png){width=672}\n:::\n:::\n\n\n**Question:** What patterns do you see? How does ridership change over time?\n\nThere is a genreal increase of ridership over time, which is most likely due to improvements in weather conditions making biking both be and feel more accessible and safe to more people. Furthermore, there is a major peak in mid-Febuary. This coincides with the Eagles' superbowl victory and corresponding parade.\n\nNotice a big spike in Februrary of 2025 (the 14th - not just Valentine's Day)? Was anybody in Philly last Feb? What closed everything and cancelled my PPA class? ... Hint: Fly Eagles Fly\n\nLet's investigate...compare the 14th to all other Friday's in the data...what did you find? Keep this in mind for future Feature Engineering.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfly_eagles_fly <- daily_trips %>% filter(date == \"2025-02-14\")\n\ntypical_boring_friday <- indego %>%\n  filter(dotw == \"Fri\", date != \"2025-02-14\") %>%\n  group_by(date) %>%\n  summarize(trips = n()) %>%\n  summarize(avg_friday_trips = mean(trips))\n\nprint(fly_eagles_fly)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  date       trips\n  <date>     <int>\n1 2025-02-14  4192\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(typical_boring_friday)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  avg_friday_trips\n             <dbl>\n1            2319.\n```\n\n\n:::\n:::\n\n\n\n## Hourly Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Average trips by hour and day type\nhourly_patterns <- indego %>%\n  group_by(hour, weekend) %>%\n  summarize(avg_trips = n() / n_distinct(date)) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/hourly_patterns-1.png){width=672}\n:::\n:::\n\n\n**Question:** When are the peak hours? How do weekends differ from weekdays?\nDuring the week, ridership peaks around 8:00am and 5:00pm. These peaks correspond with the start and end of the work day, illustrating Indego's commuting functionality. On the week, however, the stark peaks are exchanged for a more gentle curve. Peak ridership occurs around noon and the early afternoon, demonstrating a more recreational or casual usage of Indego. \n## Top Stations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Most popular origin stations\ntop_stations <- indego %>%\n  count(start_station, start_lat, start_lon, name = \"trips\") %>%\n  arrange(desc(trips)) %>%\n  head(20)\n\nkable(top_stations, \n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top 20 Indego Stations by Trip Origins</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> start_station </th>\n   <th style=\"text-align:right;\"> start_lat </th>\n   <th style=\"text-align:right;\"> start_lon </th>\n   <th style=\"text-align:right;\"> trips </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 3,010 </td>\n   <td style=\"text-align:right;\"> 39.94711 </td>\n   <td style=\"text-align:right;\"> -75.16618 </td>\n   <td style=\"text-align:right;\"> 3,999 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,032 </td>\n   <td style=\"text-align:right;\"> 39.94527 </td>\n   <td style=\"text-align:right;\"> -75.17971 </td>\n   <td style=\"text-align:right;\"> 2,842 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,359 </td>\n   <td style=\"text-align:right;\"> 39.94888 </td>\n   <td style=\"text-align:right;\"> -75.16978 </td>\n   <td style=\"text-align:right;\"> 2,699 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,020 </td>\n   <td style=\"text-align:right;\"> 39.94855 </td>\n   <td style=\"text-align:right;\"> -75.19007 </td>\n   <td style=\"text-align:right;\"> 2,673 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,208 </td>\n   <td style=\"text-align:right;\"> 39.95048 </td>\n   <td style=\"text-align:right;\"> -75.19324 </td>\n   <td style=\"text-align:right;\"> 2,503 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,244 </td>\n   <td style=\"text-align:right;\"> 39.93865 </td>\n   <td style=\"text-align:right;\"> -75.16674 </td>\n   <td style=\"text-align:right;\"> 2,486 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,066 </td>\n   <td style=\"text-align:right;\"> 39.94561 </td>\n   <td style=\"text-align:right;\"> -75.17348 </td>\n   <td style=\"text-align:right;\"> 2,396 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,362 </td>\n   <td style=\"text-align:right;\"> 39.94816 </td>\n   <td style=\"text-align:right;\"> -75.16226 </td>\n   <td style=\"text-align:right;\"> 2,387 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,012 </td>\n   <td style=\"text-align:right;\"> 39.94218 </td>\n   <td style=\"text-align:right;\"> -75.17747 </td>\n   <td style=\"text-align:right;\"> 2,361 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,028 </td>\n   <td style=\"text-align:right;\"> 39.94061 </td>\n   <td style=\"text-align:right;\"> -75.14958 </td>\n   <td style=\"text-align:right;\"> 2,348 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,161 </td>\n   <td style=\"text-align:right;\"> 39.95486 </td>\n   <td style=\"text-align:right;\"> -75.18091 </td>\n   <td style=\"text-align:right;\"> 2,278 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,101 </td>\n   <td style=\"text-align:right;\"> 39.94295 </td>\n   <td style=\"text-align:right;\"> -75.15955 </td>\n   <td style=\"text-align:right;\"> 2,274 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,295 </td>\n   <td style=\"text-align:right;\"> 39.95028 </td>\n   <td style=\"text-align:right;\"> -75.16027 </td>\n   <td style=\"text-align:right;\"> 2,160 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,054 </td>\n   <td style=\"text-align:right;\"> 39.96250 </td>\n   <td style=\"text-align:right;\"> -75.17420 </td>\n   <td style=\"text-align:right;\"> 2,123 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,185 </td>\n   <td style=\"text-align:right;\"> 39.95169 </td>\n   <td style=\"text-align:right;\"> -75.15888 </td>\n   <td style=\"text-align:right;\"> 2,116 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,038 </td>\n   <td style=\"text-align:right;\"> 39.94781 </td>\n   <td style=\"text-align:right;\"> -75.19409 </td>\n   <td style=\"text-align:right;\"> 2,111 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,203 </td>\n   <td style=\"text-align:right;\"> 39.94077 </td>\n   <td style=\"text-align:right;\"> -75.17227 </td>\n   <td style=\"text-align:right;\"> 2,106 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,059 </td>\n   <td style=\"text-align:right;\"> 39.96244 </td>\n   <td style=\"text-align:right;\"> -75.16121 </td>\n   <td style=\"text-align:right;\"> 2,027 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,022 </td>\n   <td style=\"text-align:right;\"> 39.95472 </td>\n   <td style=\"text-align:right;\"> -75.18323 </td>\n   <td style=\"text-align:right;\"> 2,014 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,063 </td>\n   <td style=\"text-align:right;\"> 39.94633 </td>\n   <td style=\"text-align:right;\"> -75.16980 </td>\n   <td style=\"text-align:right;\"> 2,014 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n---\n\n# Get Philadelphia Spatial Context\n\n## Load Philadelphia Census Data\n\nWe'll get census tract data to add demographic context to our stations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get Philadelphia census tracts\nphilly_census <- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\",\n  progress = FALSE\n) %>%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %>%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %>%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n\n# Check the data\nglimpse(philly_census)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 408\nColumns: 17\n$ GEOID                  <chr> \"42101001500\", \"42101001800\", \"42101002802\", \"4…\n$ NAME                   <chr> \"Census Tract 15; Philadelphia County; Pennsylv…\n$ Total_Pop              <dbl> 3251, 3300, 5720, 4029, 4415, 1815, 3374, 2729,…\n$ B01003_001M            <dbl> 677, 369, 796, 437, 853, 210, 480, 734, 763, 11…\n$ Med_Inc                <dbl> 110859, 114063, 78871, 61583, 32347, 48581, 597…\n$ B19013_001M            <dbl> 24975, 30714, 20396, 22293, 4840, 13812, 6278, …\n$ Total_Commuters        <dbl> 2073, 2255, 3032, 2326, 1980, 969, 2427, 708, 2…\n$ B08301_001M            <dbl> 387, 308, 478, 383, 456, 189, 380, 281, 456, 68…\n$ Transit_Commuters      <dbl> 429, 123, 685, 506, 534, 192, 658, 218, 438, 51…\n$ B08301_010M            <dbl> 188, 66, 219, 144, 285, 71, 278, 184, 176, 235,…\n$ White_Pop              <dbl> 2185, 2494, 3691, 3223, 182, 984, 2111, 231, 35…\n$ B02001_002M            <dbl> 268, 381, 592, 380, 88, 190, 463, 112, 238, 778…\n$ Med_Home_Value         <dbl> 568300, 605000, 350600, 296400, 76600, 289700, …\n$ B25077_001M            <dbl> 58894, 34876, 12572, 22333, 10843, 118720, 1506…\n$ geometry               <MULTIPOLYGON [°]> MULTIPOLYGON (((-75.16558 3..., MU…\n$ Percent_Taking_Transit <dbl> 20.694645, 5.454545, 22.592348, 21.754084, 26.9…\n$ Percent_White          <dbl> 67.2100892, 75.5757576, 64.5279720, 79.9950360,…\n```\n\n\n:::\n:::\n\n\n## Map Philadelphia Context\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = indego,\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 0.25, alpha = 0.6\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/map_philly-1.png){width=672}\n:::\n:::\n\n\n## Join Census Data to Stations\n\nWe'll spatially join census characteristics to each bike station.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sf object for stations\nstations_sf <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census <- st_join(stations_sf, philly_census, left = TRUE) %>%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_census <- indego %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations_for_map %>% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations_for_map %>% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/join_census_to_stations-1.png){width=672}\n:::\n:::\n\n\n# Dealing with missing data\n\nWe need to decide what to do with the non-residential bike share stations. For this example, we are going to remove them -- this is not necessarily the right way to do things always, but for the sake of simplicity, we are narrowing our scope to only stations in residential neighborhoods. We might opt to create a separate model for non-residential stations..\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identify which stations to keep\nvalid_stations <- stations_census %>%\n  filter(!is.na(Med_Inc)) %>%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_census <- indego %>%\n  filter(start_station %in% valid_stations) %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n```\n:::\n\n\n\n# Get Weather Data\n\nWeather significantly affects bike share demand! Let's get hourly weather for Philadelphia.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q1 2025: January 1 - March 31\nweather_data <- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2025-01-01\",\n  date_end = \"2025-03-31\"\n)\n\n# Process weather data\nweather_processed <- weather_data %>%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %>%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %>%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete <- weather_processed %>%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %>%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather_complete %>% select(Temperature, Precipitation, Wind_Speed))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Temperature    Precipitation        Wind_Speed    \n Min.   :10.00   Min.   :0.000000   Min.   : 0.000  \n 1st Qu.:30.00   1st Qu.:0.000000   1st Qu.: 5.000  \n Median :37.00   Median :0.000000   Median : 8.000  \n Mean   :38.66   Mean   :0.005459   Mean   : 9.047  \n 3rd Qu.:47.00   3rd Qu.:0.000000   3rd Qu.:12.000  \n Max.   :78.00   Max.   :0.710000   Max.   :30.000  \n```\n\n\n:::\n:::\n\n\n## Visualize Weather Patterns\n\nWho is ready for a Philly winter?!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(weather_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q1 2025\",\n    subtitle = \"Winter to early spring transition\",\n    x = \"Date\",\n    y = \"Temperature (°F)\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/visualize_weather-1.png){width=672}\n:::\n:::\n\n\n---\n\n# Create Space-Time Panel\n\n## Aggregate Trips to Station-Hour Level\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count trips by station-hour\ntrips_panel <- indego_census %>%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%\n  summarize(Trip_Count = n()) %>%\n  ungroup()\n\n# How many station-hour observations?\nnrow(trips_panel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 116718\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 245\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2150\n```\n\n\n:::\n:::\n\n\n## Create Complete Panel Structure\n\nNot every station has trips every hour. We need a **complete panel** where every station-hour combination exists (even if Trip_Count = 0).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate expected panel size\nn_stations <- length(unique(trips_panel$start_station))\nn_hours <- length(unique(trips_panel$interval60))\nexpected_rows <- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected panel rows: 526,750 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCurrent rows: 116,718 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMissing rows: 410,032 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create complete panel\nstudy_panel <- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %>%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %>%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes <- trips_panel %>%\n  group_by(start_station) %>%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel <- study_panel %>%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify we have complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nComplete panel rows: 526,750 \n```\n\n\n:::\n:::\n\n\n## Add Time Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n```\n:::\n\n\n## Join Weather Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  left_join(weather_complete, by = \"interval60\")\n\n# Check for missing values\nsummary(study_panel %>% select(Trip_Count, Temperature, Precipitation))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Trip_Count     Temperature   Precipitation   \n Min.   : 0.00   Min.   :10.0   Min.   :0.0000  \n 1st Qu.: 0.00   1st Qu.:30.0   1st Qu.:0.0000  \n Median : 0.00   Median :37.0   Median :0.0000  \n Mean   : 0.34   Mean   :38.7   Mean   :0.0055  \n 3rd Qu.: 0.00   3rd Qu.:47.0   3rd Qu.:0.0000  \n Max.   :26.00   Max.   :78.0   Max.   :0.7100  \n                 NA's   :5880   NA's   :5880    \n```\n\n\n:::\n:::\n\n\n---\n\n# Create Temporal Lag Variables\n\nThe key innovation for space-time prediction: **past demand predicts future demand**.\n\n## Why Lags?\n\nIf there were 15 bike trips from Station A at 8:00 AM, there will probably be ~15 trips at 9:00 AM. We can use this temporal persistence to improve predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort by station and time\nstudy_panel <- study_panel %>%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel <- study_panel %>%\n  group_by(start_station) %>%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %>%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete <- study_panel %>%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows after removing NA lags: 635,775 \n```\n\n\n:::\n:::\n\n\n## Visualize Lag Correlations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample one station to visualize\nexample_station <- study_panel_complete %>%\n  filter(start_station == first(start_station)) %>%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/lag_correlations-1.png){width=672}\n:::\n:::\n\n\n---\n\n# Temporal Train/Test Split\n\n**CRITICAL:** We must train on PAST data and test on FUTURE data!\n\n## Why Temporal Validation Matters\n\nIn real operations, at 6:00 AM on March 15, we need to predict demand for March 15-31. We have data from Jan 1 - March 14, but NOT from March 15-31 (it hasn't happened yet!).\n\n**Wrong approach:** Train on weeks 10-13, test on weeks 1-9 (predicting past from future!)\n\n**Correct approach:** Train on weeks 1-9, test on weeks 10-13 (predicting future from past)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split by week\n# Q1 has weeks 1-13 (Jan-Mar)\n# Train on weeks 1-9 (Jan 1 - early March)\n# Test on weeks 10-13 (rest of March)\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations <- study_panel_complete %>%\n  filter(week < 10) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations <- study_panel_complete %>%\n  filter(week >= 10) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations <- intersect(early_stations, late_stations)\n\n\n# Filter panel to only common stations\nstudy_panel_complete <- study_panel_complete %>%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain <- study_panel_complete %>%\n  filter(week < 10)\n\ntest <- study_panel_complete %>%\n  filter(week >= 10)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining observations: 428,400 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting observations: 189,210 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining date range: 20089 to 20151 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting date range: 20152 to 20178 \n```\n\n\n:::\n:::\n\n\n---\n\n# Build Predictive Models\n\nWe'll build 5 models with increasing complexity to see what improves predictions.\n\n## Model 1: Baseline (Time + Weather)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntrain <- train %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) <- contr.treatment(7)\n\n# Now run the model\nmodel1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8820 -0.3802 -0.1791  0.0128 18.5540 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.1488732  0.0077026 -19.328 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0182693  0.0078250  -2.335               0.0196 *  \nas.factor(hour)2  -0.0401185  0.0078013  -5.143   0.0000002711413737 ***\nas.factor(hour)3  -0.0534144  0.0077758  -6.869   0.0000000000064582 ***\nas.factor(hour)4  -0.0420757  0.0078905  -5.332   0.0000000969595824 ***\nas.factor(hour)5   0.0145021  0.0078916   1.838               0.0661 .  \nas.factor(hour)6   0.1551150  0.0078162  19.845 < 0.0000000000000002 ***\nas.factor(hour)7   0.2839418  0.0077451  36.661 < 0.0000000000000002 ***\nas.factor(hour)8   0.5012789  0.0077165  64.962 < 0.0000000000000002 ***\nas.factor(hour)9   0.3598210  0.0078159  46.037 < 0.0000000000000002 ***\nas.factor(hour)10  0.2460810  0.0076647  32.106 < 0.0000000000000002 ***\nas.factor(hour)11  0.2676516  0.0077151  34.692 < 0.0000000000000002 ***\nas.factor(hour)12  0.3345613  0.0077972  42.908 < 0.0000000000000002 ***\nas.factor(hour)13  0.3335884  0.0078885  42.288 < 0.0000000000000002 ***\nas.factor(hour)14  0.3298815  0.0076963  42.862 < 0.0000000000000002 ***\nas.factor(hour)15  0.3889267  0.0077546  50.155 < 0.0000000000000002 ***\nas.factor(hour)16  0.4812228  0.0077566  62.041 < 0.0000000000000002 ***\nas.factor(hour)17  0.5848969  0.0077931  75.053 < 0.0000000000000002 ***\nas.factor(hour)18  0.4003396  0.0075825  52.798 < 0.0000000000000002 ***\nas.factor(hour)19  0.2521779  0.0077095  32.710 < 0.0000000000000002 ***\nas.factor(hour)20  0.1373139  0.0074848  18.346 < 0.0000000000000002 ***\nas.factor(hour)21  0.0880114  0.0075923  11.592 < 0.0000000000000002 ***\nas.factor(hour)22  0.0580815  0.0076060   7.636   0.0000000000000224 ***\nas.factor(hour)23  0.0350222  0.0076227   4.594   0.0000043403255736 ***\ndotw_simple2       0.0651434  0.0043665  14.919 < 0.0000000000000002 ***\ndotw_simple3       0.0430540  0.0043119   9.985 < 0.0000000000000002 ***\ndotw_simple4       0.0505750  0.0041306  12.244 < 0.0000000000000002 ***\ndotw_simple5       0.0434119  0.0041584  10.440 < 0.0000000000000002 ***\ndotw_simple6      -0.0672427  0.0042085 -15.978 < 0.0000000000000002 ***\ndotw_simple7      -0.0631357  0.0041631 -15.166 < 0.0000000000000002 ***\nTemperature        0.0069235  0.0001365  50.722 < 0.0000000000000002 ***\nPrecipitation     -2.4416620  0.0939158 -25.998 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7216 on 428368 degrees of freedom\nMultiple R-squared:  0.07598,\tAdjusted R-squared:  0.07592 \nF-statistic:  1136 on 31 and 428368 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\nThe model uses Monday as the baseline. Each coefficient represents the difference \nin expected trips per station-hour compared to Monday - dow_simple2 = Tuesday..\n\n**Weekday Pattern (Tue-Fri):**\n\n- All weekdays have positive coefficients (0.029 to 0.052)\n- Tuesday has the highest weekday effect (+0.052)\n- Weekdays likely benefit from concentrated commuting patterns\n\n**Weekend Pattern (Sat-Sun):**\n\n- Both weekend days have negative coefficients (-0.061 and -0.065)\n- This means FEWER trips per station-hour than Monday\n\n**Hourly Interpretation**\n\nHour   Coefficient   Interpretation\n0      (baseline)    0.000 trips/hour (midnight)\n1      -0.018       slightly fewer than midnight\n...\n6      +0.151       morning activity starting\n7      +0.276       morning rush building\n8      +0.487       PEAK morning rush\n9      +0.350       post-rush\n...\n17     +0.568       PEAK evening rush (5 PM!)\n18     +0.389       evening declining\n...\n23     +0.034       late night minimal\n\nIsn't this fun!\n\n## Model 2: Add Temporal Lags\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1487 -0.2595 -0.0970  0.0235 17.5255 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.0944031  0.0069331 -13.616 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0028329  0.0070412  -0.402             0.687436    \nas.factor(hour)2  -0.0113676  0.0070205  -1.619             0.105403    \nas.factor(hour)3  -0.0233665  0.0069984  -3.339             0.000841 ***\nas.factor(hour)4  -0.0014341  0.0071021  -0.202             0.839977    \nas.factor(hour)5   0.0463719  0.0071041   6.528 0.000000000066943682 ***\nas.factor(hour)6   0.1484482  0.0070401  21.086 < 0.0000000000000002 ***\nas.factor(hour)7   0.2157558  0.0069815  30.904 < 0.0000000000000002 ***\nas.factor(hour)8   0.3617314  0.0069664  51.925 < 0.0000000000000002 ***\nas.factor(hour)9   0.1603371  0.0070640  22.698 < 0.0000000000000002 ***\nas.factor(hour)10  0.1024525  0.0069124  14.822 < 0.0000000000000002 ***\nas.factor(hour)11  0.1360277  0.0069632  19.535 < 0.0000000000000002 ***\nas.factor(hour)12  0.2021688  0.0070303  28.757 < 0.0000000000000002 ***\nas.factor(hour)13  0.1900074  0.0071126  26.714 < 0.0000000000000002 ***\nas.factor(hour)14  0.1963837  0.0069380  28.305 < 0.0000000000000002 ***\nas.factor(hour)15  0.2435050  0.0069932  34.820 < 0.0000000000000002 ***\nas.factor(hour)16  0.3042270  0.0070018  43.450 < 0.0000000000000002 ***\nas.factor(hour)17  0.3749605  0.0070436  53.234 < 0.0000000000000002 ***\nas.factor(hour)18  0.1792367  0.0068606  26.126 < 0.0000000000000002 ***\nas.factor(hour)19  0.0973764  0.0069608  13.989 < 0.0000000000000002 ***\nas.factor(hour)20  0.0389727  0.0067549   5.770 0.000000007955345969 ***\nas.factor(hour)21  0.0320507  0.0068394   4.686 0.000002784344470639 ***\nas.factor(hour)22  0.0310219  0.0068459   4.531 0.000005859991631025 ***\nas.factor(hour)23  0.0221611  0.0068594   3.231             0.001235 ** \ndotw_simple2       0.0271522  0.0039314   6.907 0.000000000004972450 ***\ndotw_simple3       0.0147801  0.0038816   3.808             0.000140 ***\ndotw_simple4       0.0206099  0.0037187   5.542 0.000000029877813199 ***\ndotw_simple5       0.0127648  0.0037443   3.409             0.000652 ***\ndotw_simple6      -0.0521272  0.0037909 -13.750 < 0.0000000000000002 ***\ndotw_simple7      -0.0303227  0.0037485  -8.089 0.000000000000000602 ***\nTemperature        0.0029026  0.0001236  23.486 < 0.0000000000000002 ***\nPrecipitation     -1.3850445  0.0846392 -16.364 < 0.0000000000000002 ***\nlag1Hour           0.3247918  0.0014556 223.135 < 0.0000000000000002 ***\nlag3Hours          0.0975134  0.0014354  67.936 < 0.0000000000000002 ***\nlag1day            0.1659090  0.0013907 119.300 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6493 on 428365 degrees of freedom\nMultiple R-squared:  0.2519,\tAdjusted R-squared:  0.2518 \nF-statistic:  4241 on 34 and 428365 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n**Question:** Did adding lags improve R²? Why or why not?\nI believe adding lags improved the model for two main reasons. Primarily, lags improve the theoretical aspects of the model, as I believe ridership — especially in the previous hour and day — are incredibly related to expected ridership of the hour we hope to estimate. Furthermore, this theoretical connection is found in the slightly improved R2. Whilst R2 is not the end all be all, the increase from .07 to .25 is extremely notable.\n\n## Model 3: Add Demographics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\nsummary(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0188 -0.4829 -0.2267  0.2771 16.9058 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.9225844734  0.0352000581  26.210\nas.factor(hour)1          0.0037772168  0.0441610229   0.086\nas.factor(hour)2         -0.0955075476  0.0487102008  -1.961\nas.factor(hour)3         -0.1340354597  0.0623637654  -2.149\nas.factor(hour)4         -0.1416017281  0.0554770480  -2.552\nas.factor(hour)5         -0.1300062643  0.0388982897  -3.342\nas.factor(hour)6          0.0360772463  0.0331031436   1.090\nas.factor(hour)7          0.1328270695  0.0317248940   4.187\nas.factor(hour)8          0.3866084171  0.0308539302  12.530\nas.factor(hour)9         -0.0445315405  0.0310696933  -1.433\nas.factor(hour)10        -0.0847033567  0.0313859062  -2.699\nas.factor(hour)11        -0.0290106009  0.0314061584  -0.924\nas.factor(hour)12         0.0385416417  0.0310913865   1.240\nas.factor(hour)13         0.0421197178  0.0311715057   1.351\nas.factor(hour)14         0.0117664453  0.0307921034   0.382\nas.factor(hour)15         0.0826672722  0.0306447205   2.698\nas.factor(hour)16         0.1940771623  0.0304962350   6.364\nas.factor(hour)17         0.3048041726  0.0304046610  10.025\nas.factor(hour)18         0.0002652992  0.0305035090   0.009\nas.factor(hour)19        -0.0634666243  0.0312457890  -2.031\nas.factor(hour)20        -0.1392336862  0.0318719442  -4.369\nas.factor(hour)21        -0.1321162902  0.0327955752  -4.028\nas.factor(hour)22        -0.1054260206  0.0336780868  -3.130\nas.factor(hour)23        -0.0569114464  0.0351511626  -1.619\ndotw_simple2              0.0229986016  0.0122313741   1.880\ndotw_simple3             -0.0175461556  0.0122262223  -1.435\ndotw_simple4             -0.0442688138  0.0115791517  -3.823\ndotw_simple5              0.0027339310  0.0117287820   0.233\ndotw_simple6             -0.1108106236  0.0125102844  -8.858\ndotw_simple7             -0.0398429023  0.0126913144  -3.139\nTemperature               0.0039381085  0.0003773197  10.437\nPrecipitation            -4.4263561137  0.3064039720 -14.446\nlag1Hour                  0.2204571054  0.0028934647  76.191\nlag3Hours                 0.0583071946  0.0031516027  18.501\nlag1day                   0.1447413119  0.0029839179  48.507\nMed_Inc.x                 0.0000004207  0.0000001117   3.767\nPercent_Taking_Transit.y -0.0011705604  0.0004209381  -2.781\nPercent_White.y           0.0018451795  0.0002099438   8.789\n                                     Pr(>|t|)    \n(Intercept)              < 0.0000000000000002 ***\nas.factor(hour)1                     0.931838    \nas.factor(hour)2                     0.049914 *  \nas.factor(hour)3                     0.031617 *  \nas.factor(hour)4                     0.010699 *  \nas.factor(hour)5                     0.000832 ***\nas.factor(hour)6                     0.275785    \nas.factor(hour)7               0.000028315277 ***\nas.factor(hour)8         < 0.0000000000000002 ***\nas.factor(hour)9                     0.151782    \nas.factor(hour)10                    0.006961 ** \nas.factor(hour)11                    0.355633    \nas.factor(hour)12                    0.215118    \nas.factor(hour)13                    0.176627    \nas.factor(hour)14                    0.702369    \nas.factor(hour)15                    0.006985 ** \nas.factor(hour)16              0.000000000198 ***\nas.factor(hour)17        < 0.0000000000000002 ***\nas.factor(hour)18                    0.993061    \nas.factor(hour)19                    0.042237 *  \nas.factor(hour)20              0.000012523294 ***\nas.factor(hour)21              0.000056188260 ***\nas.factor(hour)22                    0.001746 ** \nas.factor(hour)23                    0.105441    \ndotw_simple2                         0.060071 .  \ndotw_simple3                         0.151255    \ndotw_simple4                         0.000132 ***\ndotw_simple5                         0.815688    \ndotw_simple6             < 0.0000000000000002 ***\ndotw_simple7                         0.001694 ** \nTemperature              < 0.0000000000000002 ***\nPrecipitation            < 0.0000000000000002 ***\nlag1Hour                 < 0.0000000000000002 ***\nlag3Hours                < 0.0000000000000002 ***\nlag1day                  < 0.0000000000000002 ***\nMed_Inc.x                            0.000165 ***\nPercent_Taking_Transit.y             0.005423 ** \nPercent_White.y          < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9136 on 83854 degrees of freedom\n  (344508 observations deleted due to missingness)\nMultiple R-squared:  0.1706,\tAdjusted R-squared:  0.1702 \nF-statistic:   466 on 37 and 83854 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n## Model 4: Add Station Fixed Effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 R-squared: 0.1922707 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 Adj R-squared: 0.1896529 \n```\n\n\n:::\n:::\n\n\n**What do station fixed effects capture?** Baseline differences in demand across stations (some are just busier than others!).\n\n## Model 5: Add Rush Hour Interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel5 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 R-squared: 0.1968772 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 Adj R-squared: 0.1942455 \n```\n\n\n:::\n:::\n\n\n---\n\n# Model Evaluation\n\n## Calculate Predictions and MAE\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions on test set\n\n# Create day of week factor with treatment (dummy) coding\ntest <- test %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) <- contr.treatment(7)\n\ntest <- test %>%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results <- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.60 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.50 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.74 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.73 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.73 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Visualize Model Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mae_results, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/compare_models-1.png){width=672}\n:::\n:::\n\n\n**Question:** Which features gave us the biggest improvement?\n\nThe temporal lag model recorded the least mean absolute error, illustrating that perhaps recent ridership is the best predicting factor for future ridership. This makes some sense, as recent ridership inherently takes into account the time/weather, rush hour, demographics, and the qualities of the station via the *free market of riders*.\n\n---\n\n# Space-Time Error Analysis\n\n## Observed vs. Predicted\n\nLet's use our best model (Model 2) for error analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 2 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/obs_vs_pred-1.png){width=672}\n:::\n:::\n\n\n**Question:** Where is the model performing well? Where is it struggling?\nThe model appears to preforms best for the AM rush and evening for weekdays as well as for the PM rush on both weekdays and weekends. The model struggles the most at midday for both weekdays and weekends. Performance is measured by relative alignment to the *perfect predictions* line.\n\n\n\n## Spatial Error Patterns\n\nAre prediction errors clustered in certain parts of Philadelphia?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MAE by station\nstation_errors <- test %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)\n\n# Calculate station errors\nstation_errors <- test %>%\n  filter(!is.na(pred2)) %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE\\n(trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),  # Fewer, cleaner breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Higher in Center City\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand\np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg\\nDemand\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),  # Clear breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\",\n       subtitle = \"Trips per station-hour\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine with better layout\nlibrary(gridExtra)\nlibrary(grid)\ngrid.arrange(\n  p1, p2, \n  ncol = 2,\n  top = textGrob(\n    \"Model 2 Performance: Errors vs. Demand Patterns\",\n    gp = gpar(fontsize = 16, fontface = \"bold\")\n  )\n)\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/spatial_errors-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/spatial_errors-2.png){width=672}\n:::\n\n```{.r .cell-code}\np1\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/spatial_errors-3.png){width=672}\n:::\n:::\n\n\n**Question:** Do you see spatial clustering of errors? What neighborhoods have high errors?\nThere is a cluster of errosr along the the Schuykill River as it passes through Manayunk and East Falls, the western end of Center City, as well as along the Delaware River (Society Hill?). The errors seem to mimic stations/areas that have higher than average \"average trips/hour\". \n\n\n## Temporal Error Patterns\n\nWhen are we most wrong?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MAE by time of day and day type\ntemporal_errors <- test %>%\n  group_by(time_of_day, weekend) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/temporal_errors-1.png){width=672}\n:::\n:::\n\n\n## Errors and Demographics\n\nAre prediction errors related to neighborhood characteristics?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join demographic data to station errors\nstation_errors_demo <- station_errors %>%\n  left_join(\n    station_attributes %>% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %>%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/errors_demographics-1.png){width=672}\n:::\n:::\n\n\n**Critical Question:** Are prediction errors systematically higher in certain demographic groups? What are the equity implications?\n\nPrediciton error has a postive trend with an increase in both median income and percentage of white residents. Error also negative relationship with transit ridership. Whiter, wealthier areas of the city tend to be denser and near center city or other economic hub, which are where Indego stations are more densely located. It could be thought that wealthier and whiter areas of the city have greater options when deciding which Indego bike station to use, whereas more diverse and poorer areas of the city have limited options if any at all.\n---\n\n# HW5: Your Turn! \n\nFor Homework 5, you'll work either all byyyy yourself or in teams of 2 to:\n\n## Part 1: Replicate with Different Quarter (alternately, you could do a longer time-span by merging multiple quarters together. I'm not picky about that)\n\nI chose Q2 of 2025 because the weather will be warmer but not too warm to bike.\n\n---\n\n# Data Import & Preparation\n\n## Load Indego Trip Data (Q2 2025)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#get current directory (not same as project home)\n\n# Read Q2 2025 data\n\nindego <- read_csv(here(\"data/indego-trips-2025-q2.csv\"))\n\n# Quick look at the data\nglimpse(indego)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 365,760\nColumns: 15\n$ trip_id             <dbl> 1164246461, 1164246634, 1164246553, 1164246521, 11…\n$ duration            <dbl> 11, 31, 9, 3, 11, 7, 13, 12, 3, 58, 21, 5, 15, 10,…\n$ start_time          <chr> \"4/1/2025 0:04\", \"4/1/2025 0:04\", \"4/1/2025 0:17\",…\n$ end_time            <chr> \"4/1/2025 0:15\", \"4/1/2025 0:35\", \"4/1/2025 0:26\",…\n$ start_station       <dbl> 3022, 3040, 3396, 3054, 3280, 3301, 3158, 3374, 33…\n$ start_lat           <dbl> 39.95472, 39.96289, 39.92327, 39.96250, 39.93968, …\n$ start_lon           <dbl> -75.18323, -75.16606, -75.18210, -75.17420, -75.21…\n$ end_station         <dbl> 3064, 3100, 3349, 3235, 3349, 3051, 3028, 3154, 33…\n$ end_lat             <dbl> 39.93840, 39.92777, 39.93651, 39.96000, 39.93651, …\n$ end_lon             <dbl> -75.17327, -75.15103, -75.18621, -75.16510, -75.18…\n$ bike_id             <chr> \"31751\", \"14481\", \"02724\", \"24841\", \"25744\", \"3123…\n$ plan_duration       <dbl> 30, 30, 30, 365, 30, 365, 1, 30, 30, 30, 30, 30, 1…\n$ trip_route_category <chr> \"One Way\", \"One Way\", \"One Way\", \"One Way\", \"One W…\n$ passholder_type     <chr> \"Indego30\", \"Indego30\", \"Indego30\", \"Indego365\", \"…\n$ bike_type           <chr> \"electric\", \"standard\", \"standard\", \"electric\", \"e…\n```\n\n\n:::\n:::\n\n\n## Examine the Data Structure\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many trips?\ncat(\"Total trips in Q1 2025:\", nrow(indego), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal trips in Q1 2025: 365760 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego$start_time)), \"to\", \n    max(mdy_hm(indego$start_time)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDate range: 1743465840 to 1751327820 \n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego$start_station)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique start stations: 273 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Trip types\ntable(indego$trip_route_category)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   One Way Round Trip \n    341060      24700 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Passholder types\ntable(indego$passholder_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  Day Pass   Indego30  Indego365 IndegoFlex       NULL    Walk-up \n     15712     189135     132693          1          4      28215 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Bike types\ntable(indego$bike_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nelectric standard \n  234502   131258 \n```\n\n\n:::\n:::\n\n\n## Create Time Bins\n\nWe need to aggregate trips into hourly intervals for our panel data structure.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindego <- indego %>%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego %>% select(start_datetime, interval60, week, dotw, hour, weekend))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  <dttm>              <dttm>              <dbl> <ord> <int>   <dbl>\n1 2025-04-01 00:04:00 2025-04-01 00:00:00    13 Tue       0       0\n2 2025-04-01 00:04:00 2025-04-01 00:00:00    13 Tue       0       0\n3 2025-04-01 00:17:00 2025-04-01 00:00:00    13 Tue       0       0\n4 2025-04-01 00:20:00 2025-04-01 00:00:00    13 Tue       0       0\n5 2025-04-01 00:25:00 2025-04-01 00:00:00    13 Tue       0       0\n6 2025-04-01 00:40:00 2025-04-01 00:00:00    13 Tue       0       0\n```\n\n\n:::\n:::\n\n\n---\n\n# Exploratory Analysis\n\n## Trips Over Time\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daily trip counts\ndaily_trips <- indego %>%\n  group_by(date) %>%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q1 2025\",\n    subtitle = \"Winter demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/trips_over_time2-1.png){width=672}\n:::\n:::\n\n\n\n\n## Hourly Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Average trips by hour and day type\nhourly_patterns <- indego %>%\n  group_by(hour, weekend) %>%\n  summarize(avg_trips = n() / n_distinct(date)) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/hourly_patterns2-1.png){width=672}\n:::\n:::\n\n\n## Top Stations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Most popular origin stations\ntop_stations <- indego %>%\n  count(start_station, start_lat, start_lon, name = \"trips\") %>%\n  arrange(desc(trips)) %>%\n  head(20)\n\nkable(top_stations, \n      caption = \"Top 20 Indego Stations by Trip Origins\",\n      format.args = list(big.mark = \",\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Top 20 Indego Stations by Trip Origins</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> start_station </th>\n   <th style=\"text-align:right;\"> start_lat </th>\n   <th style=\"text-align:right;\"> start_lon </th>\n   <th style=\"text-align:right;\"> trips </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 3,010 </td>\n   <td style=\"text-align:right;\"> 39.94711 </td>\n   <td style=\"text-align:right;\"> -75.16618 </td>\n   <td style=\"text-align:right;\"> 6,278 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,032 </td>\n   <td style=\"text-align:right;\"> 39.94527 </td>\n   <td style=\"text-align:right;\"> -75.17971 </td>\n   <td style=\"text-align:right;\"> 4,751 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,359 </td>\n   <td style=\"text-align:right;\"> 39.94888 </td>\n   <td style=\"text-align:right;\"> -75.16978 </td>\n   <td style=\"text-align:right;\"> 4,181 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,028 </td>\n   <td style=\"text-align:right;\"> 39.94061 </td>\n   <td style=\"text-align:right;\"> -75.14958 </td>\n   <td style=\"text-align:right;\"> 4,028 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,163 </td>\n   <td style=\"text-align:right;\"> 39.94974 </td>\n   <td style=\"text-align:right;\"> -75.18097 </td>\n   <td style=\"text-align:right;\"> 4,000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,020 </td>\n   <td style=\"text-align:right;\"> 39.94855 </td>\n   <td style=\"text-align:right;\"> -75.19007 </td>\n   <td style=\"text-align:right;\"> 3,946 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,185 </td>\n   <td style=\"text-align:right;\"> 39.95169 </td>\n   <td style=\"text-align:right;\"> -75.15888 </td>\n   <td style=\"text-align:right;\"> 3,914 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,066 </td>\n   <td style=\"text-align:right;\"> 39.94561 </td>\n   <td style=\"text-align:right;\"> -75.17348 </td>\n   <td style=\"text-align:right;\"> 3,899 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,022 </td>\n   <td style=\"text-align:right;\"> 39.95472 </td>\n   <td style=\"text-align:right;\"> -75.18323 </td>\n   <td style=\"text-align:right;\"> 3,856 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,054 </td>\n   <td style=\"text-align:right;\"> 39.96250 </td>\n   <td style=\"text-align:right;\"> -75.17420 </td>\n   <td style=\"text-align:right;\"> 3,767 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,161 </td>\n   <td style=\"text-align:right;\"> 39.95486 </td>\n   <td style=\"text-align:right;\"> -75.18091 </td>\n   <td style=\"text-align:right;\"> 3,629 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,362 </td>\n   <td style=\"text-align:right;\"> 39.94816 </td>\n   <td style=\"text-align:right;\"> -75.16226 </td>\n   <td style=\"text-align:right;\"> 3,552 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,063 </td>\n   <td style=\"text-align:right;\"> 39.94633 </td>\n   <td style=\"text-align:right;\"> -75.16980 </td>\n   <td style=\"text-align:right;\"> 3,516 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,213 </td>\n   <td style=\"text-align:right;\"> 39.93887 </td>\n   <td style=\"text-align:right;\"> -75.16663 </td>\n   <td style=\"text-align:right;\"> 3,332 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,059 </td>\n   <td style=\"text-align:right;\"> 39.96244 </td>\n   <td style=\"text-align:right;\"> -75.16121 </td>\n   <td style=\"text-align:right;\"> 3,320 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,012 </td>\n   <td style=\"text-align:right;\"> 39.94218 </td>\n   <td style=\"text-align:right;\"> -75.17747 </td>\n   <td style=\"text-align:right;\"> 3,264 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,007 </td>\n   <td style=\"text-align:right;\"> 39.94517 </td>\n   <td style=\"text-align:right;\"> -75.15993 </td>\n   <td style=\"text-align:right;\"> 3,242 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,061 </td>\n   <td style=\"text-align:right;\"> 39.95425 </td>\n   <td style=\"text-align:right;\"> -75.17761 </td>\n   <td style=\"text-align:right;\"> 3,193 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,101 </td>\n   <td style=\"text-align:right;\"> 39.94295 </td>\n   <td style=\"text-align:right;\"> -75.15955 </td>\n   <td style=\"text-align:right;\"> 3,170 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3,296 </td>\n   <td style=\"text-align:right;\"> 39.95134 </td>\n   <td style=\"text-align:right;\"> -75.16758 </td>\n   <td style=\"text-align:right;\"> 3,088 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n---\n\n# Get Philadelphia Spatial Context\n\n## Load Philadelphia Census Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get Philadelphia census tracts\nphilly_census <- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\",\n  progress = FALSE\n) %>%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %>%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %>%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n\n# Check the data\nglimpse(philly_census)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 408\nColumns: 17\n$ GEOID                  <chr> \"42101001500\", \"42101001800\", \"42101002802\", \"4…\n$ NAME                   <chr> \"Census Tract 15; Philadelphia County; Pennsylv…\n$ Total_Pop              <dbl> 3251, 3300, 5720, 4029, 4415, 1815, 3374, 2729,…\n$ B01003_001M            <dbl> 677, 369, 796, 437, 853, 210, 480, 734, 763, 11…\n$ Med_Inc                <dbl> 110859, 114063, 78871, 61583, 32347, 48581, 597…\n$ B19013_001M            <dbl> 24975, 30714, 20396, 22293, 4840, 13812, 6278, …\n$ Total_Commuters        <dbl> 2073, 2255, 3032, 2326, 1980, 969, 2427, 708, 2…\n$ B08301_001M            <dbl> 387, 308, 478, 383, 456, 189, 380, 281, 456, 68…\n$ Transit_Commuters      <dbl> 429, 123, 685, 506, 534, 192, 658, 218, 438, 51…\n$ B08301_010M            <dbl> 188, 66, 219, 144, 285, 71, 278, 184, 176, 235,…\n$ White_Pop              <dbl> 2185, 2494, 3691, 3223, 182, 984, 2111, 231, 35…\n$ B02001_002M            <dbl> 268, 381, 592, 380, 88, 190, 463, 112, 238, 778…\n$ Med_Home_Value         <dbl> 568300, 605000, 350600, 296400, 76600, 289700, …\n$ B25077_001M            <dbl> 58894, 34876, 12572, 22333, 10843, 118720, 1506…\n$ geometry               <MULTIPOLYGON [°]> MULTIPOLYGON (((-75.16558 3..., MU…\n$ Percent_Taking_Transit <dbl> 20.694645, 5.454545, 22.592348, 21.754084, 26.9…\n$ Percent_White          <dbl> 67.2100892, 75.5757576, 64.5279720, 79.9950360,…\n```\n\n\n:::\n:::\n\n\n## Map Philadelphia Context\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = indego,\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 0.25, alpha = 0.6\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/map_philly2-1.png){width=672}\n:::\n:::\n\n\n## Join Census Data to Stations\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sf object for stations\nstations_sf <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census <- st_join(stations_sf, philly_census, left = TRUE) %>%\n  st_drop_geometry()\n\n# Look at the result - investigate whether all of the stations joined to census data -- according to the map above there are stations in non-residential tracts.\n\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_census <- indego %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n# Prepare data for visualization\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data (small grey dots)\n  geom_point(\n    data = stations_for_map %>% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data (red X marks the spot)\n  geom_point(\n    data = stations_for_map %>% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Indego stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/join_census_to_stations2-1.png){width=672}\n:::\n:::\n\n\n# Dealing with missing data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identify which stations to keep\nvalid_stations <- stations_census %>%\n  filter(!is.na(Med_Inc)) %>%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_census <- indego %>%\n  filter(start_station %in% valid_stations) %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n```\n:::\n\n\n\n# Get Weather Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q1 2025: April 1 - June 30\nweather_data <- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2025-04-01\",\n  date_end = \"2025-06-30\"\n)\n\n# Process weather data\nweather_processed <- weather_data %>%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %>%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %>%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete <- weather_processed %>%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %>%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather_complete %>% select(Temperature, Precipitation, Wind_Speed))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Temperature     Precipitation       Wind_Speed    \n Min.   : 33.00   Min.   :0.00000   Min.   : 0.000  \n 1st Qu.: 57.00   1st Qu.:0.00000   1st Qu.: 5.000  \n Median : 65.00   Median :0.00000   Median : 8.000  \n Mean   : 64.69   Mean   :0.01121   Mean   : 8.103  \n 3rd Qu.: 72.00   3rd Qu.:0.00010   3rd Qu.:10.000  \n Max.   :100.00   Max.   :1.14000   Max.   :40.000  \n```\n\n\n:::\n:::\n\n\n## Visualize Weather Patterns\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(weather_complete, aes(x = interval60, y = Temperature)) +\n  geom_line(color = \"#3182bd\", alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    title = \"Philadelphia Temperature - Q2 2025\",\n    subtitle = \"Winter to early spring transition\",\n    x = \"Date\",\n    y = \"Temperature (°F)\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/visualize_weather2-1.png){width=672}\n:::\n:::\n\n\n---\n\n# Create Space-Time Panel\n\n## Aggregate Trips to Station-Hour Level\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count trips by station-hour\ntrips_panel <- indego_census %>%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%\n  summarize(Trip_Count = n()) %>%\n  ungroup()\n\n# How many station-hour observations?\nnrow(trips_panel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 179107\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\nlength(unique(trips_panel$start_station))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 252\n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique hours?\nlength(unique(trips_panel$interval60))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2177\n```\n\n\n:::\n:::\n\n\n## Create Complete Panel Structure\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate expected panel size\nn_stations <- length(unique(trips_panel$start_station))\nn_hours <- length(unique(trips_panel$interval60))\nexpected_rows <- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected panel rows: 548,604 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCurrent rows: 179,107 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMissing rows: 369,497 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create complete panel\nstudy_panel <- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %>%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %>%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes <- trips_panel %>%\n  group_by(start_station) %>%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel <- study_panel %>%\n  left_join(station_attributes, by = \"start_station\")\n\n# Verify we have complete panel\ncat(\"Complete panel rows:\", format(nrow(study_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nComplete panel rows: 548,604 \n```\n\n\n:::\n:::\n\n\n## Add Time Features\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n```\n:::\n\n\n## Join Weather Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  left_join(weather_complete, by = \"interval60\")\n\n# Check for missing values\nsummary(study_panel %>% select(Trip_Count, Temperature, Precipitation))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Trip_Count       Temperature    Precipitation   \n Min.   : 0.0000   Min.   : 33.0   Min.   :0.0000  \n 1st Qu.: 0.0000   1st Qu.: 57.0   1st Qu.:0.0000  \n Median : 0.0000   Median : 65.0   Median :0.0000  \n Mean   : 0.6055   Mean   : 64.7   Mean   :0.0111  \n 3rd Qu.: 1.0000   3rd Qu.: 72.0   3rd Qu.:0.0001  \n Max.   :27.0000   Max.   :100.0   Max.   :1.1400  \n                   NA's   :6048    NA's   :6048    \n```\n\n\n:::\n:::\n\n\n---\n\n# Create Temporal Lag Variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort by station and time\nstudy_panel <- study_panel %>%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel <- study_panel %>%\n  group_by(start_station) %>%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %>%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete <- study_panel %>%\n  filter(!is.na(lag1day))\n\ncat(\"Rows after removing NA lags:\", format(nrow(study_panel_complete), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows after removing NA lags: 727,776 \n```\n\n\n:::\n:::\n\n\n## Visualize Lag Correlations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample one station to visualize\nexample_station <- study_panel_complete %>%\n  filter(start_station == first(start_station)) %>%\n  head(168)  # One week\n\n# Plot actual vs lagged demand\nggplot(example_station, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour, color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day, color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\" = \"#08519c\",\n    \"1 Hour Ago\" = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station\",\n    subtitle = \"Past demand predicts future demand\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/lag_correlations2-1.png){width=672}\n:::\n:::\n\n\n---\n\n# Temporal Train/Test Split\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split by week\n# Q1 has weeks 14-26 (April-June)\n# Train on weeks 14-23 (Apr 1 - early June)\n# Test on weeks 23-26 (rest of June)\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations <- study_panel_complete %>%\n  filter(week < 23) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations <- study_panel_complete %>%\n  filter(week >= 23) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations <- intersect(early_stations, late_stations)\n\n\n# Filter panel to only common stations\nstudy_panel_complete <- study_panel_complete %>%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain <- study_panel_complete %>%\n  filter(week < 23)\n\ntest <- study_panel_complete %>%\n  filter(week >= 23)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining observations: 498,747 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting observations: 220,365 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining date range: 20179 to 20242 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting date range: 20243 to 20269 \n```\n\n\n:::\n:::\n\n\n---\n\n# Build Predictive Models\n\nWe'll build 5 models with increasing complexity to see what improves predictions.\n\n## Model 1: Baseline (Time + Weather)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntrain <- train %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) <- contr.treatment(7)\n\n# Now run the model\nmodel1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\n\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5973 -0.6602 -0.2101  0.2056 25.5474 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.6377352  0.0135543 -47.050 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0411708  0.0110414  -3.729             0.000192 ***\nas.factor(hour)2  -0.0639198  0.0111100  -5.753   0.0000000087548969 ***\nas.factor(hour)3  -0.1045973  0.0111883  -9.349 < 0.0000000000000002 ***\nas.factor(hour)4  -0.0857670  0.0111916  -7.664   0.0000000000000181 ***\nas.factor(hour)5   0.0136237  0.0110296   1.235             0.216759    \nas.factor(hour)6   0.2345878  0.0112929  20.773 < 0.0000000000000002 ***\nas.factor(hour)7   0.4903620  0.0110199  44.498 < 0.0000000000000002 ***\nas.factor(hour)8   0.8226332  0.0109747  74.957 < 0.0000000000000002 ***\nas.factor(hour)9   0.6066977  0.0109072  55.624 < 0.0000000000000002 ***\nas.factor(hour)10  0.4822213  0.0109270  44.131 < 0.0000000000000002 ***\nas.factor(hour)11  0.5251388  0.0107154  49.008 < 0.0000000000000002 ***\nas.factor(hour)12  0.5449794  0.0108905  50.042 < 0.0000000000000002 ***\nas.factor(hour)13  0.5447782  0.0107258  50.791 < 0.0000000000000002 ***\nas.factor(hour)14  0.5785329  0.0106672  54.235 < 0.0000000000000002 ***\nas.factor(hour)15  0.6899547  0.0107275  64.317 < 0.0000000000000002 ***\nas.factor(hour)16  0.8418613  0.0107744  78.136 < 0.0000000000000002 ***\nas.factor(hour)17  1.1067064  0.0106875 103.552 < 0.0000000000000002 ***\nas.factor(hour)18  0.8956767  0.0110737  80.883 < 0.0000000000000002 ***\nas.factor(hour)19  0.6214480  0.0108481  57.286 < 0.0000000000000002 ***\nas.factor(hour)20  0.3792801  0.0110807  34.229 < 0.0000000000000002 ***\nas.factor(hour)21  0.2415679  0.0110556  21.850 < 0.0000000000000002 ***\nas.factor(hour)22  0.1619637  0.0113139  14.315 < 0.0000000000000002 ***\nas.factor(hour)23  0.0797368  0.0108465   7.351   0.0000000000001964 ***\ndotw_simple2       0.0447468  0.0060084   7.447   0.0000000000000954 ***\ndotw_simple3      -0.0390441  0.0059647  -6.546   0.0000000000592243 ***\ndotw_simple4       0.0367432  0.0058631   6.267   0.0000000003686729 ***\ndotw_simple5      -0.0190780  0.0058151  -3.281             0.001035 ** \ndotw_simple6      -0.0395519  0.0060522  -6.535   0.0000000000636339 ***\ndotw_simple7      -0.0593200  0.0062383  -9.509 < 0.0000000000000002 ***\nTemperature        0.0135446  0.0001686  80.354 < 0.0000000000000002 ***\nPrecipitation     -0.0830165  0.0215151  -3.859             0.000114 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.104 on 498715 degrees of freedom\nMultiple R-squared:  0.108,\tAdjusted R-squared:  0.1079 \nF-statistic:  1947 on 31 and 498715 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n## Model 2: Add Temporal Lags\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.7651  -0.4068  -0.1187   0.1067  20.1518 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.2468884  0.0115469 -21.381 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0134365  0.0093749  -1.433             0.151789    \nas.factor(hour)2  -0.0040216  0.0094359  -0.426             0.669962    \nas.factor(hour)3  -0.0300068  0.0095050  -3.157             0.001594 ** \nas.factor(hour)4  -0.0126009  0.0095152  -1.324             0.185410    \nas.factor(hour)5   0.0673110  0.0093788   7.177    0.000000000000714 ***\nas.factor(hour)6   0.2357904  0.0096053  24.548 < 0.0000000000000002 ***\nas.factor(hour)7   0.3775938  0.0093781  40.263 < 0.0000000000000002 ***\nas.factor(hour)8   0.5690617  0.0093518  60.851 < 0.0000000000000002 ***\nas.factor(hour)9   0.2389521  0.0093037  25.684 < 0.0000000000000002 ***\nas.factor(hour)10  0.1833591  0.0093034  19.709 < 0.0000000000000002 ***\nas.factor(hour)11  0.2430645  0.0091259  26.635 < 0.0000000000000002 ***\nas.factor(hour)12  0.2813190  0.0092694  30.349 < 0.0000000000000002 ***\nas.factor(hour)13  0.2944059  0.0091260  32.260 < 0.0000000000000002 ***\nas.factor(hour)14  0.3142945  0.0090776  34.623 < 0.0000000000000002 ***\nas.factor(hour)15  0.3951557  0.0091321  43.271 < 0.0000000000000002 ***\nas.factor(hour)16  0.4930111  0.0091816  53.695 < 0.0000000000000002 ***\nas.factor(hour)17  0.6811937  0.0091256  74.647 < 0.0000000000000002 ***\nas.factor(hour)18  0.3717998  0.0094783  39.227 < 0.0000000000000002 ***\nas.factor(hour)19  0.2065849  0.0092634  22.301 < 0.0000000000000002 ***\nas.factor(hour)20  0.0622124  0.0094515   6.582    0.000000000046375 ***\nas.factor(hour)21  0.0511982  0.0094059   5.443    0.000000052351743 ***\nas.factor(hour)22  0.0461062  0.0096124   4.797    0.000001614798692 ***\nas.factor(hour)23  0.0272482  0.0092090   2.959             0.003088 ** \ndotw_simple2       0.0195096  0.0051011   3.825             0.000131 ***\ndotw_simple3      -0.0345530  0.0050665  -6.820    0.000000000009121 ***\ndotw_simple4       0.0214327  0.0049775   4.306    0.000016632428861 ***\ndotw_simple5      -0.0122444  0.0049371  -2.480             0.013136 *  \ndotw_simple6      -0.0215405  0.0051397  -4.191    0.000027780809914 ***\ndotw_simple7      -0.0388226  0.0052980  -7.328    0.000000000000234 ***\nTemperature        0.0038062  0.0001452  26.215 < 0.0000000000000002 ***\nPrecipitation     -0.1105215  0.0182675  -6.050    0.000000001448012 ***\nlag1Hour           0.4196719  0.0013114 320.017 < 0.0000000000000002 ***\nlag3Hours          0.1219574  0.0012950  94.173 < 0.0000000000000002 ***\nlag1day            0.1301294  0.0011988 108.550 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9374 on 498712 degrees of freedom\nMultiple R-squared:  0.3571,\tAdjusted R-squared:  0.3571 \nF-statistic:  8149 on 34 and 498712 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n## Model 3: Add Demographics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\nsummary(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.2209 -0.6616 -0.2563  0.4035 20.4632 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.4406970415  0.0363319876  12.130\nas.factor(hour)1          0.0496543983  0.0387077576   1.283\nas.factor(hour)2          0.0557316649  0.0413301325   1.348\nas.factor(hour)3         -0.0854994429  0.0526259771  -1.625\nas.factor(hour)4         -0.0506592117  0.0502917111  -1.007\nas.factor(hour)5          0.0249121749  0.0351554261   0.709\nas.factor(hour)6          0.3080255762  0.0306174303  10.060\nas.factor(hour)7          0.4102664374  0.0283084868  14.493\nas.factor(hour)8          0.6237282228  0.0273206080  22.830\nas.factor(hour)9          0.1229538278  0.0274924099   4.472\nas.factor(hour)10         0.1089465034  0.0278635955   3.910\nas.factor(hour)11         0.1415417179  0.0273330360   5.178\nas.factor(hour)12         0.2058031351  0.0274404196   7.500\nas.factor(hour)13         0.2423025788  0.0272442395   8.894\nas.factor(hour)14         0.1998194065  0.0269288510   7.420\nas.factor(hour)15         0.3146303978  0.0268079668  11.736\nas.factor(hour)16         0.4608457292  0.0267343133  17.238\nas.factor(hour)17         0.7033251763  0.0264420906  26.599\nas.factor(hour)18         0.2792961266  0.0269051672  10.381\nas.factor(hour)19         0.0955337184  0.0270520220   3.531\nas.factor(hour)20        -0.0243192000  0.0279745479  -0.869\nas.factor(hour)21        -0.0248597751  0.0287195075  -0.866\nas.factor(hour)22        -0.0135141145  0.0298326902  -0.453\nas.factor(hour)23        -0.0034317764  0.0304173530  -0.113\ndotw_simple2              0.0323640406  0.0114869781   2.817\ndotw_simple3             -0.0598277266  0.0118480871  -5.050\ndotw_simple4             -0.0088472033  0.0113540073  -0.779\ndotw_simple5             -0.0690814729  0.0112102533  -6.162\ndotw_simple6              0.0356817125  0.0119532435   2.985\ndotw_simple7              0.0059532202  0.0123577029   0.482\nTemperature               0.0074256468  0.0003481431  21.329\nPrecipitation            -0.3450355405  0.0361701753  -9.539\nlag1Hour                  0.3101259828  0.0021643730 143.287\nlag3Hours                 0.0882016341  0.0022259445  39.624\nlag1day                   0.1149544429  0.0021229555  54.148\nMed_Inc.x                 0.0000002947  0.0000001086   2.714\nPercent_Taking_Transit.y -0.0040106436  0.0004076555  -9.838\nPercent_White.y           0.0025740005  0.0002007487  12.822\n                                     Pr(>|t|)    \n(Intercept)              < 0.0000000000000002 ***\nas.factor(hour)1                     0.199563    \nas.factor(hour)2                     0.177515    \nas.factor(hour)3                     0.104237    \nas.factor(hour)4                     0.313789    \nas.factor(hour)5                     0.478555    \nas.factor(hour)6         < 0.0000000000000002 ***\nas.factor(hour)7         < 0.0000000000000002 ***\nas.factor(hour)8         < 0.0000000000000002 ***\nas.factor(hour)9           0.0000077443208911 ***\nas.factor(hour)10          0.0000923370105501 ***\nas.factor(hour)11          0.0000002240580230 ***\nas.factor(hour)12          0.0000000000000642 ***\nas.factor(hour)13        < 0.0000000000000002 ***\nas.factor(hour)14          0.0000000000001175 ***\nas.factor(hour)15        < 0.0000000000000002 ***\nas.factor(hour)16        < 0.0000000000000002 ***\nas.factor(hour)17        < 0.0000000000000002 ***\nas.factor(hour)18        < 0.0000000000000002 ***\nas.factor(hour)19                    0.000413 ***\nas.factor(hour)20                    0.384666    \nas.factor(hour)21                    0.386708    \nas.factor(hour)22                    0.650552    \nas.factor(hour)23                    0.910171    \ndotw_simple2                         0.004841 ** \ndotw_simple3               0.0000004433034819 ***\ndotw_simple4                         0.435855    \ndotw_simple5               0.0000000007184786 ***\ndotw_simple6                         0.002835 ** \ndotw_simple7                         0.629990    \nTemperature              < 0.0000000000000002 ***\nPrecipitation            < 0.0000000000000002 ***\nlag1Hour                 < 0.0000000000000002 ***\nlag3Hours                < 0.0000000000000002 ***\nlag1day                  < 0.0000000000000002 ***\nMed_Inc.x                            0.006652 ** \nPercent_Taking_Transit.y < 0.0000000000000002 ***\nPercent_White.y          < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.211 on 156977 degrees of freedom\n  (341732 observations deleted due to missingness)\nMultiple R-squared:  0.2487,\tAdjusted R-squared:  0.2486 \nF-statistic:  1405 on 37 and 156977 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\n## Model 4: Add Station Fixed Effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 R-squared: 0.2718325 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 Adj R-squared: 0.2705223 \n```\n\n\n:::\n:::\n\n\n\n\n## Model 5: Add Rush Hour Interaction\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel5 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 R-squared: 0.2759937 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 Adj R-squared: 0.2746771 \n```\n\n\n:::\n:::\n\n\n---\n\n# Model Evaluation\n\n## Calculate Predictions and MAE\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions on test set\n\n# Create day of week factor with treatment (dummy) coding\ntest <- test %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) <- contr.treatment(7)\n\ntest <- test %>%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results <- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.82 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.62 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.86 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.86 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.90 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Visualize Model Comparison\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(mae_results, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/compare_models-2-1.png){width=672}\n:::\n:::\n\n\n---\n\n\n\n\n **Compare results** to Q1 2025:\n\n   - How do MAE values compare? Why might they differ?\n   - Are temporal patterns different (e.g., summer vs. winter)?\n   - Which features are most important in your quarter?\n\nThe 2025 Q2 MAE values are larger across all time periods, which most likely can be attributed to the larger amount of cyclists during this period (slightly more than 36k vs slightly more than 20k). The better weather definitely has something to do with this, as it invites less bike-dependent/craved individuals to use Indego. These additional casual cyclists are less predictable or stable.\n\n## Part 2: Error Analysis \n\nAnalyze your model's errors in detail:\n\n\n# Space-Time Error Analysis\n\n## Observed vs. Predicted\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    )\n  )\n\n# Scatter plot by time and day type\nggplot(test, aes(x = Trip_Count, y = pred2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips\",\n    subtitle = \"Model 2 performance by time period\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/obs_vs_pred2-1.png){width=672}\n:::\n:::\n\n\n\n## Spatial Error Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate MAE by station\nstation_errors <- test %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)\n\n# Calculate station errors\nstation_errors <- test %>%\n  filter(!is.na(pred2)) %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE\\n(trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),  # Fewer, cleaner breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\",\n       subtitle = \"Higher in Center City\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand\np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg\\nDemand\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),  # Clear breaks\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\",\n       subtitle = \"Trips per station-hour\") +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 10)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 1.5,\n    barheight = 12,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine with better layout\nlibrary(gridExtra)\nlibrary(grid)\ngrid.arrange(\n  p1, p2, \n  ncol = 2,\n  top = textGrob(\n    \"Model 2 Performance: Errors vs. Demand Patterns\",\n    gp = gpar(fontsize = 16, fontface = \"bold\")\n  )\n)\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/spatial_errors2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/spatial_errors2-2.png){width=672}\n:::\n\n```{.r .cell-code}\np1\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/spatial_errors2-3.png){width=672}\n:::\n:::\n\n\n\n## Temporal Error Patterns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MAE by time of day and day type\ntemporal_errors <- test %>%\n  group_by(time_of_day, weekend) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/temporal_errors2-1.png){width=672}\n:::\n:::\n\n\n## Errors and Demographics\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join demographic data to station errors\nstation_errors_demo <- station_errors %>%\n  left_join(\n    station_attributes %>% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %>%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](lab5_files/figure-html/errors_demographics2-1.png){width=672}\n:::\n:::\n\n\n\n1. **Spatial patterns:**\n\nThe highest average errors are once again mirroring the stations with the highest average demand, This is most likely because these areas are both commute and post-work destinations so the patterns are dependent on the work day. Furthermore, with work from home becoming ore popular, not every weekday can be classified equally.\n\n2. **Temporal patterns:**\n\nThe errors are highest during the am and pm rush hours during the weekdays. This is most likely because these times are when the most people are reliant on the Indego network. The high reliance in these times means that people in the densely commuted to areas of the city (which are populated with a plethora of Indego stations) may not always use the same station or a station at all (i.e. staying in center city for a work event, running errands to skip the evening or morning rush, etc.).\n\n\n3. **Demographic patterns:**\n\nWhite, Wealthy, and/or transit-heavy communities are still the hardest to predict, which, as previously stated, is most likely due to the wide array of Indego stations accessible to those communities. Thus, the equity implications stems from the easier to predict minority and lower-income communities, as it is clear they are offered *and deserve* an equal ability to choose Indego stations. The lack of stations in these communities limits their ability to have options, use the system conveniently, and access the larger bicycle network.\n\n\n\n\n## Part 4: Critical Reflection \n\nWrite 1-2 paragraphs addressing:\n\n1. **Operational implications:**\n   - Is your final MAE \"good enough\" for Indego to use?\n   - When do prediction errors cause problems for rebalancing?\n   - Would you recommend deploying this system? Under what conditions?\n\n2. **Equity considerations:**\n   - Do prediction errors disproportionately affect certain neighborhoods?\n   - Could this system worsen existing disparities in bike access?\n   - What safeguards would you recommend?\n\n3. **Model limitations:**\n   - What patterns is your model missing?\n   - What assumptions might not hold in real deployment?\n   - How would you improve this with more time/data?\n\nI do not believe the final MAE is \"*good enough*\" for Indego. Although MAE always being below 1 seems good, it is important to remember the scale of the panel data: 1 hour. Being a trip off per hour on average is huge when considering some Indego stations have as little as 10 bikes docks. This is especially risky during high-usage times, where MAE can increase above 1 during the evening rush hour on weekdays. Furthermore, MAE tends to increase as it gets into areas that rely heavily on transit, are wealthier, and whiter, meaning areas such as center city (which might be the most important point in the Indego network due to it being a commuting hub) are even more likely to be wrong. The model is missing patterns of the school calendar. universities and high schools might have a lot of students use Indego to commute. Summer break and the outflow of students from Philadelphia may subvert the expected relation that increased temperature correlates to increased ridership. Thus, I would not consider this model ready for deployment. With more time, I would consider factoring in school calendars as well as proximity to bike lanes, as I believe feelings of safety are crucial for increased ridership.\n\n---\n\n# Submission Requirements\n\n## What to Submit (per team)\n\n1. **Rmd file** with all your code (commented!)\n2. **HTML output** with results and visualizations\n3. **Brief report** summarizing (with supporting data & visualization):\n\n   - Your quarter and why you chose it\n   - Model comparison results\n   - Error analysis insights\n   - New features you added and why\n   - Critical reflection on deployment\n\n## Tips for Success\n\n1. **Start early** - data download and processing takes time\n2. **Work together** - pair programming is your friend\n3. **Test incrementally** - don't wait until the end to run code\n4. **Document everything** - explain your choices\n5. **Be creative** - the best features come from understanding Philly!\n6. **Think critically** - technical sophistication isn't enough\n\n---\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}